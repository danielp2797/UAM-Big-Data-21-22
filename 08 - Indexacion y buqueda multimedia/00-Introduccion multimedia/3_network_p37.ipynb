{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbrPXkSGlcmB"
      },
      "source": [
        "#Máster en Big Data y Data Science: Ciencia e Ingeniería de Datos\n",
        "### ASIGNATURA: Indexación, búsqueda y análisis en repositorios multimedia\n",
        "### PARTE: Multimedia (imagen, video)\n",
        "### Práctica 1: Introducción al diseño de redes neuronales convolucionales con Pytorch mediante Google Colaboratory\n",
        "\n",
        "---\n",
        "\n",
        "Autor: Juan C. SanMiguel (juancarlos.sanmiguel@uam.es), Universidad Autónoma de Madrid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7_EXhmb8zwb"
      },
      "source": [
        "# 3. Definición de la red neuronal convolucional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zoOE4V7l5tS"
      },
      "source": [
        "## Preparación del entorno de trabajo\n",
        "\n",
        "A continuación tiene un conjunto de instrucciones que instalan el software necesario para esta parte de la práctica.\n",
        "\n",
        "Recuerde que este código es compatible con Python 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm_31ZAnl8wl",
        "outputId": "7c3f9edc-bdef-452d-deec-e3268e43e610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip3 install torch==1.10.0+cu111 torchvision==0.11.1+cu111 torchaudio==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp38-cp38-linux_x86_64.whl (2137.6 MB)\n",
            "\u001b[K     |████████████▌                   | 834.1 MB 1.3 MB/s eta 0:16:12tcmalloc: large alloc 1147494400 bytes == 0x3a1e4000 @  0x7fe55c3f7615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |███████████████▉                | 1055.7 MB 1.3 MB/s eta 0:14:17tcmalloc: large alloc 1434370048 bytes == 0x7e83a000 @  0x7fe55c3f7615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |████████████████████            | 1336.2 MB 1.4 MB/s eta 0:09:28tcmalloc: large alloc 1792966656 bytes == 0x366c000 @  0x7fe55c3f7615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |█████████████████████████▎      | 1691.1 MB 1.2 MB/s eta 0:06:06tcmalloc: large alloc 2241208320 bytes == 0x6e454000 @  0x7fe55c3f7615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |████████████████████████████████| 2137.6 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 2137636864 bytes == 0xf3db6000 @  0x7fe55c3f61e7 0x4d30a0 0x4d312c 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91\n",
            "tcmalloc: large alloc 2672050176 bytes == 0x1e78e0000 @  0x7fe55c3f7615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91 0x5d8941 0x4fe318\n",
            "\u001b[K     |████████████████████████████████| 2137.6 MB 6.8 kB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/torchvision/\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.11.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp38-cp38-linux_x86_64.whl (24.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.5 MB 89.1 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchaudio-0.10.0%2Bcu111-cp38-cp38-linux_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.0+cu111) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.1+cu111) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.1+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.0+cu116\n",
            "    Uninstalling torchvision-0.14.0+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.0+cu116\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.0+cu116\n",
            "    Uninstalling torchaudio-0.13.0+cu116:\n",
            "      Successfully uninstalled torchaudio-0.13.0+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.10.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0+cu111 torchaudio-0.10.0+cu111 torchvision-0.11.1+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vqnHhs1cUOJ"
      },
      "source": [
        "## Red Convolucional Neuronal (CNN)\n",
        "\n",
        "En esta parte vamos a describir los elementos básicos para definir una red neuronal de tipo *feed-forward*. Este tipo de redes toman una serie de datos de entrada (*input*), éstos son procesados por una serie de capas (*layers*) de manera secuencial y finalmente se genera una salida (*output*) relacionada con la tarea a resolver.\n",
        "\n",
        "En general, para definir la mayoría de redes convolucionales necesitamos conocer los siguientes pasos:\n",
        "\n",
        "*   Definir la estructura de la red, que tendrá algunos parámetros entrenables (i.e. *weights*).\n",
        "*   Definir la secuencia de procesado de los datos para obtener una salida (i.e. *forward pass*).\n",
        "*   Calcular la función de pérdidas que indique la precisión de la salida con respecto a nuestras anotaciones (i.e. *loss function*).\n",
        "*   Calcular los gradientes de retropropagación ejecutando en modo inverso la red (i.e. *backward propagation*)\n",
        "*   Actualización de los parámetros de la red acorde a los gradientes anteriores\n",
        "\n",
        "En esta parte, vamos a tomar como ejemplo la red LENET http://yann.lecun.com/exdb/lenet/ cuya estructura se visualiza a continuacion:\n",
        "\n",
        "\n",
        "![alt text](http://pytorch.org/tutorials/_images/mnist.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K05iyB_ykEjj"
      },
      "source": [
        "\n",
        "### Capas de una red (layers)\n",
        "Para definir las capas una red, Pytorch hace uso del paquete ``torch.nn``.\n",
        "\n",
        "Primeramente podemos definir *capas convolucionales 2D* mediante la función ``nn.Conv2d`` que tiene los principales argumentos:\n",
        "*   **in_channels**: valor para la tercera dimension del tensor de entrada (e.g. 3 para imágenes RGB, 1 para imágenes en Gris). \n",
        "*   **out_channels**: número de mapas de salida (i.e. número de convoluciones o *kernels* que aplicamos sobre los datos de entrada).\n",
        "*   **kernel_size**: tamaño del *kernel* aplicado (tamaño x tamaño)\n",
        "*   **stride**: desplazamiento de la aplicación del operador de convolución\n",
        "*   **padding**: tipo de padding applicado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOqUpwekkyQZ",
        "outputId": "c8c7f6a2-c1ab-4cb4-9105-06465b3bd1c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "conv1 = nn.Conv2d(1, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
        "\n",
        "print(conv1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJIlDKF2l5lZ"
      },
      "source": [
        "Posteriormente tenemos *capas con conexión completa* (*fully connected*) mediante la función ``nn.Linear``. Como comparación con la capa convolucional, en esta se asume que todas los datos de entrada están conectados a todos los datos de salida, hecho que aumenta significativamente el número de parámetros. La función tiene los principales argumentos:\n",
        "*   **in_features**: numero de unidades de entrada. \n",
        "*   **out_features**: numero de unidades de entrada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcjtXoj6mQHZ",
        "outputId": "d6d1b290-8ed8-4750-d1a7-d74f68980ba7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "fc1 = nn.Linear(in_features=240, out_features=120)\n",
        "\n",
        "print(fc1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=240, out_features=120, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnPxR0_nmutN"
      },
      "source": [
        "También exite una etapa dedicada a reducir la dimensionalidad de los datos, cuya nomenclatura es ``nn.MaxPool2d``. Tiene los siguientes argumentos de interés:\n",
        "\n",
        "*   **kernel_size**: tamaño del *kernel* aplicado (tamaño x tamaño)\n",
        "*   **stride**: desplazamiento de la aplicación del operador de convolución\n",
        "*   **padding**: tipo de padding applicado\n",
        "\n",
        "Es importante resaltar el efecto de esta etapa. Por ejemplo con *kernel_size=2* y *stride=2*, estaremos reduciendo la dimensionalidad de la imagen por dos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9q5LZ3DnLry",
        "outputId": "e9449a37-074b-475c-e84f-6aa4ff2986b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "print(pool1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w9gOWoBoKjW"
      },
      "source": [
        "Por último se destaca la operación de activación, implementada mayoritariamente mediante funciones ``ReLU``. En el siguiente código se encuentra comentada debido a que, para ser efectiva, necesita datos 'x' a procesar. Estos datos 'x' se corresponderán con tensor, que no utilizan/cargan en el siguiente bloque de código."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdBqua3OoY5m",
        "outputId": "33fc2ce5-a21f-4b72-be09-c0c824bc4395",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "fc1 = nn.Linear(in_features=240, out_features=120)\n",
        "#x = F.relu(fc1(x)) #fc1(x) corresponde con aplicar la capa fc1 a unos datos 'x'\n",
        "\n",
        "print(fc1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=240, out_features=120, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW-MovTyk3-q"
      },
      "source": [
        "### Definición red completa\n",
        "Tras las definiciones anteriores, estamos en condición de mostrar un esquema de la red LENET. \n",
        "\n",
        "Una definición básica requiere (al menos) implementar dos funciones:\n",
        "\n",
        "\n",
        "*   **__init__(self)**: indica el tipo de capas existentes en la red\n",
        "*   **forward**: indica el flujo de datos, es decir, la secuencia de ejecución cuando llegan nuevos datos a la entrada.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l7o03z8kD1m"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Convolutional layer 1\n",
        "        # 1-channel input image, 6 output channels, 5x5 square convolution \n",
        "        self.conv1 = nn.Conv2d(1, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
        "        \n",
        "        # Max pooling over a (2, 2) window\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        # Convolutional layer 2\n",
        "        # 6-channel input data, 16 output channels, 5x5 square convolution \n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5) \n",
        "        \n",
        "        # Fully connected neural network layers \n",
        "        # implement an affine operation: y = Wx + b\n",
        "        # where x-input\n",
        "        #       y-output\n",
        "        #       W-weights\n",
        "        #       b-bias\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120) #fully connected layer 1\n",
        "        self.fc2 = nn.Linear(120, 84)         #fully connected layer 2\n",
        "        self.fc3 = nn.Linear(84, 10)          #fully connected layer 3\n",
        "        \n",
        "        #weight initializacion\n",
        "        #...\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        #layer 1: conv + ReLU + pooling\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        #x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # definicion alternativa\n",
        "\n",
        "        #layer 2: conv + ReLU + pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        #x = F.max_pool2d(F.relu(self.conv2(x)), 2) #  definicion alternativa\n",
        "\n",
        "        #flatten data: convert 2D data into a 1D column vector\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        \n",
        "        #layer 3: fully connected\n",
        "        x = F.relu(self.fc1(x))\n",
        "        \n",
        "        #layer 4: fully connected\n",
        "        x = F.relu(self.fc2(x))\n",
        "        \n",
        "        #layer 5: fully connected\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92BcHzrjqjvY"
      },
      "source": [
        "NOTA1: observe que *out_channels* de la capa ``conv1`` debe coincidir con la variable *in_channels* de la capa ``conv2``.\n",
        "\n",
        "NOTA2: similarmente, la salida de la capa ``fc1`` coincide con la entrada de la capa ``fc2``, cuya salida tambien coincide con la entrada de la capa ``fc3``\n",
        "\n",
        "NOTA3: observe que la salida de la capa ``fc3`` es 10, el número de clases del problema de clasificación que se quiere resolver con la red LENET\n",
        "\n",
        "\n",
        "Por último, para utilizar esta red debemos crear un objeto de ella:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCkgCBrxqksQ"
      },
      "source": [
        "net = Net()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNrNgG6gqxm0"
      },
      "source": [
        "Adicionalmente podemos las capas que componen la red creada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCAflIStq2ga",
        "outputId": "946a2534-a456-40bc-884e-fc7303d75074",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(net)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssmo9JBssNzA"
      },
      "source": [
        "Y también podemos ver los parámetros que se pueden aprender mediante entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SaimAl7sTqz",
        "outputId": "2fe715ab-3e3e-47ee-c204-c10422f5dd7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight\n",
        "print(params[2].size())  # conv2's .weight\n",
        "print(params[4].size())  # fc1's .weight\n",
        "print(params[6].size())  # fc2's .weight\n",
        "print(params[8].size())  # fc3's .weight"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "torch.Size([6, 1, 5, 5])\n",
            "torch.Size([16, 6, 5, 5])\n",
            "torch.Size([120, 400])\n",
            "torch.Size([84, 120])\n",
            "torch.Size([10, 84])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUkD4nHDsgSX"
      },
      "source": [
        "### Ejecución (forward pass)\n",
        "\n",
        "La ejecución de una red es sencilla:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT5b9VrOslRW",
        "outputId": "96b3d128-14d0-4ebb-9c05-fd88c60a4044",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "#create random data with 32x32 dimenssions\n",
        "input = Variable(torch.randn(1, 1, 32, 32))\n",
        "\n",
        "#process the data with the network\n",
        "out = net(input)\n",
        "\n",
        "#visualize network output\n",
        "print(out)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0002, -0.0508,  0.0008,  0.0157, -0.0195,  0.1097, -0.0829, -0.0361,\n",
            "         -0.0079,  0.0302]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRmZxod7tjPX"
      },
      "source": [
        "Cuando deba ejecutar su red, considere los siguientes puntos:\n",
        "\n",
        "\n",
        "*   Tamaño de entrada esperado: la red que acabamos de definir (Lenet) procesa imágenes de tamaño 32x32. Es decir, si quiere utilizar otro dataset deberá redimensionar las imágenes de entrada.\n",
        "*   *torch.nn* solamente procesa mini-batches de datos/imágenes (no imágenes individuales). Por ejemplo la función *nn.Conv2d* toma como entrada un tensor 4D ``nSamples x nChannels x Height x Width``\n",
        "*   La función **Variable** convierte los datos a procesar (tensores) en estructuras enriquecidas que permiten funcionalidades avanzadas (e.g. historial operaciones).\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm72BAOKs_Yq"
      },
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBI7veQJuwFb"
      },
      "source": [
        "### Función de pérdidas (loss function)\n",
        "\n",
        "Una función de pérdida toma el par de entradas (salida, objetivo) y calcula un valor que calcula qué tan lejos está la salida del objetivo. Existen varias [funciones de pérdida](http://pytorch.org/docs/nn.html#loss-functions) en el paquete *nn*.\n",
        "\n",
        "Una pérdida simple es: `` nn.MSELoss`` que calcula el error cuadrático medio entre la entrada y el objetivo.\n",
        "Una pérdida simple es: `` nn.CrossEntropyLoss`` que calcula el error entropía cruzada entre la entrada y el objetivo.\n",
        "\n",
        "A continuación se muestra un ejemplo de ejecución\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFCOWcBNu4QQ",
        "outputId": "09877643-72d9-452d-b31f-fec4837b19e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#generate fake input/output\n",
        "output = net(input)\n",
        "target = Variable(torch.arange(1, 11))  # a dummy target, for example\n",
        "target = target.to(torch.float32)\n",
        "\n",
        "#define criterion for loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "#apply loss function\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(38.5253, grad_fn=<MseLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K70IjjOFs_C2"
      },
      "source": [
        "### Retropropagación (backpropagation)\n",
        "\n",
        "Para poder calcular el error cometido por la red en cada etapa, debemos ejecutar la red en modo inverso mediante la funcion `` loss.backward() ``. Esta función se calcular a partir de la función `` forward `` que hemos proporcionado en la definición de la red. \n",
        "\n",
        "Para retropropagar el error, han de seguirse los siguientes pasos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxhGvQfHxfKP",
        "outputId": "2386806e-2b75-43d4-e45c-f4e90236fecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# You need to clear the existing gradients though, else gradients will be accumulated to existing gradients\n",
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "#have a look at conv1's bias gradients before and after the backward.\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.bias.grad before backward\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad after backward\n",
            "tensor([ 0.0248,  0.0170,  0.1831, -0.0651,  0.0211,  0.0467])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA1qm-42xs2l"
      },
      "source": [
        "### Actualización de parámetros/pesos (update the weights)\n",
        "\n",
        "En esta parte procedemos a explicar muy brevement el proceso de actualización de los pesos.\n",
        "\n",
        "Primeramente necesitamos una herramienta de optimización. La ténica más sencilla y comúnmente utilizada es el \"descenso estocástico por gradiente\" (Stochastic Gradient\n",
        "Descent, SGD):\n",
        "\n",
        "     ``weight = weight - learning_rate * gradient``\n",
        "     \n",
        "Que podemos implementar fácilmente en Python\n",
        "\n",
        "    learning_rate = 0.01\n",
        "    for f in net.parameters():\n",
        "        f.data.sub_(f.grad.data * learning_rate)\n",
        "        \n",
        "No obstante, existen otras aproximaciones para el proceso de optimización: SGD, Nesterov-SGD, Adam, RMSProp,... que están contenidas en el paquete ``torch.optim``. El siguiente código muestra un ejemplo de utilización del optimizador.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuQ636VtxxPN"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBllBDwhy8iS"
      },
      "source": [
        "## Bonus: definición de red con entrada genérica\n",
        "Como se ha comentado anteriormente, la red Lenet solamente puede trabajar con imágenes de gris y tamaño 32x32. En el siguiente bloque de código se muestra una adaptación de la red para procesar imágenes de mayor tamaño (en el ejemplo 224x224) y con varios canales de color (en el ejemplo 3). Observe que se ha añadido la función  ``_get_conv_output`` para calcular las dimensiones de los datos a la entrada de la capa fc1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmRjmTMtzE8g"
      },
      "source": [
        "#Original Lenet network\n",
        "#https://github.com/kuangliu/pytorch-cifar/tree/master/models\n",
        "\n",
        "#Possible extensions\n",
        "#https://discuss.pytorch.org/t/inferring-shape-via-flatten-operator/138/3\n",
        "#https://stackoverflow.com/questions/42479902/how-view-method-works-for-tensor-in-torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  \n",
        "    #define the structure of the network\n",
        "    def __init__(self, input_shape=(3, 224, 224),num_outputs=15):\n",
        "        super(Net, self).__init__()\n",
        "        # Convolutional layer - 3 input image channel, 6 output channels, 5x5 square convolution \n",
        "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=6, kernel_size=5, stride=1, padding=0)\n",
        "        \n",
        "        # Max pooling over a (2, 2) window\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        # Convolutional layer - 6 input data channel, 16 output channels, 5x5 square convolution \n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)       \n",
        "\n",
        "        n_size = self._get_conv_output(input_shape)\n",
        "        \n",
        "        # Fully connected neural network layers       \n",
        "        self.fc1 = nn.Linear(in_features=n_size, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120,    out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84,     out_features=num_outputs)\n",
        "\n",
        "    #define how data flows through the network\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        \n",
        "        #The view function is meant to reshape the tensor (flatten operator).\n",
        "        x = x.view( x.size(0),-1)\n",
        "                \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x \n",
        "        \n",
        "    def _get_conv_output(self, shape):\n",
        "        bs = 1\n",
        "        x = Variable(torch.rand(bs, *shape))\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        output_feat = self.pool(F.relu(self.conv2(x)))\n",
        "               \n",
        "        n_size = output_feat.data.view(bs, -1).size(1)\n",
        "        return n_size"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx95-m6B1fX7"
      },
      "source": [
        "A continuación, podemos crear una red para un problema dado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buKPAQwj1gYc",
        "outputId": "65815720-58ae-43d0-c121-d73178ddebf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class_names = ('clase1', 'clase2', 'clase3')\n",
        "\n",
        "#creamos una red con \n",
        "# input = imágenes RGB de tamaño 128x128\n",
        "# output = tres clases\n",
        "net = Net(input_shape=(3,128,128), num_outputs=len(class_names))\n",
        "  \n",
        "print(net)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=13456, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlSreI5J1tcb"
      },
      "source": [
        "Si cambiamos el problema, las dimensiones de la red cambian como puede observarse tras ejecutar ``print``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip3gY-Ct1xZ1",
        "outputId": "254042fa-87ba-4d69-8259-54e157ed8d4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "      \n",
        "classes = ('clase1', 'clase2', 'clase3', 'clase4', 'clase5')\n",
        "\n",
        "#creamos una red con \n",
        "# input = imágenes RGB de tamaño 128x128\n",
        "# output = tres clases\n",
        "net = Net(input_shape=(3,512,512), num_outputs=len(classes))\n",
        "  \n",
        "print(net)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=250000, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WfcH5guPUwJb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}