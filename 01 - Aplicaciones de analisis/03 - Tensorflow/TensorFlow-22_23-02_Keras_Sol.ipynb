{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"title\">Aplicaciones de Análisis &ndash; Keras</div>\n",
    "<div class=\"subtitle\">Máster en Big Data y Data Science</div>\n",
    "<div class=\"author\">Carlos María Alaíz Gudín - Universidad Autónoma de Madrid</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuración inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<head><link rel=\"stylesheet\" href=\"style.css\"></head>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<head><link rel=\"stylesheet\" href=\"style.css\"></head>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mset_memory_growth(device, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m     26\u001b[0m matplotlib\u001b[38;5;241m.\u001b[39mrc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m\"\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     27\u001b[0m matplotlib\u001b[38;5;241m.\u001b[39mrc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcividis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import gym\n",
    "\n",
    "matplotlib.rc(\"figure\", figsize=(15, 5))\n",
    "matplotlib.rc(\"image\", cmap=\"cividis\")\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primeros pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Keras es una API de alto nivel que facilita las aplicaciones de ML.\n",
    "* En la práctica, muchas veces no hay necesidad de acceder directamente a TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repaso de Redes Neuronales Artificiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las Redes Neuronales Artificiales (ANN) son modelos bioinspirados.\n",
    "\n",
    "<img src=\"figures/Neuron.svg\">\n",
    "\n",
    "* La unidad elemental es el perceptrón, o TLU.\n",
    "\n",
    "<img src=\"figures/TLU.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: el perceptrón para regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aunque tiene otros *backends*, Keras se puede importar como parte de TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Keras permite crear un modelo, equivalente al perceptrón de regresión (un modelo lineal) de forma sencilla, con métodos `fit` y `predict` asociados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([keras.layers.Dense(units=1,\n",
    "                                             activation=\"linear\",\n",
    "                                             input_shape=(1,),\n",
    "                                             name=\"TLU\")])\n",
    "model.compile(loss=\"mean_squared_error\")\n",
    "\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Entrenar el perceptrón sobre el conjunto de datos (`x_tr`, `y_tr`) usado anteriormente.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 26.7595\n",
      "Epoch 2/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.6792\n",
      "Epoch 3/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.6173\n",
      "Epoch 4/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.5614\n",
      "Epoch 5/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 26.5101\n",
      "Epoch 6/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.4648\n",
      "Epoch 7/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.4185\n",
      "Epoch 8/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.3702\n",
      "Epoch 9/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.3221\n",
      "Epoch 10/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 26.2781\n",
      "Epoch 11/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 26.2347\n",
      "Epoch 12/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.1907\n",
      "Epoch 13/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.1515\n",
      "Epoch 14/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.1079\n",
      "Epoch 15/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.0639\n",
      "Epoch 16/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 26.0181\n",
      "Epoch 17/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.9765\n",
      "Epoch 18/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.9355\n",
      "Epoch 19/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 25.8919\n",
      "Epoch 20/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 25.8467\n",
      "Epoch 21/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.8027\n",
      "Epoch 22/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.7598\n",
      "Epoch 23/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.7174\n",
      "Epoch 24/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.6783\n",
      "Epoch 25/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.6351\n",
      "Epoch 26/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.5926\n",
      "Epoch 27/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.5472\n",
      "Epoch 28/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.5004\n",
      "Epoch 29/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 25.4599\n",
      "Epoch 30/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 25.4208\n",
      "Epoch 31/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.3775\n",
      "Epoch 32/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.3333\n",
      "Epoch 33/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.2899\n",
      "Epoch 34/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.2471\n",
      "Epoch 35/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.2034\n",
      "Epoch 36/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.1614\n",
      "Epoch 37/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.1223\n",
      "Epoch 38/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.0760\n",
      "Epoch 39/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 25.0327\n",
      "Epoch 40/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.9883\n",
      "Epoch 41/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.9470\n",
      "Epoch 42/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.9038\n",
      "Epoch 43/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.8586\n",
      "Epoch 44/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 24.8174\n",
      "Epoch 45/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.7759\n",
      "Epoch 46/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.7304\n",
      "Epoch 47/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.6866\n",
      "Epoch 48/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.6436\n",
      "Epoch 49/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.6026\n",
      "Epoch 50/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.5610\n",
      "Epoch 51/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.5161\n",
      "Epoch 52/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.4722\n",
      "Epoch 53/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.4287\n",
      "Epoch 54/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.3886\n",
      "Epoch 55/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.3508\n",
      "Epoch 56/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.3079\n",
      "Epoch 57/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.2644\n",
      "Epoch 58/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 24.2223\n",
      "Epoch 59/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.1827\n",
      "Epoch 60/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.1440\n",
      "Epoch 61/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.1028\n",
      "Epoch 62/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.0587\n",
      "Epoch 63/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 24.0195\n",
      "Epoch 64/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.9779\n",
      "Epoch 65/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 23.9350\n",
      "Epoch 66/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 23.8924\n",
      "Epoch 67/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.8519\n",
      "Epoch 68/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.8125\n",
      "Epoch 69/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 23.7680\n",
      "Epoch 70/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 23.7250\n",
      "Epoch 71/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.6845\n",
      "Epoch 72/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 23.6432\n",
      "Epoch 73/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.6021\n",
      "Epoch 74/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.5584\n",
      "Epoch 75/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 23.5170\n",
      "Epoch 76/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 23.4728\n",
      "Epoch 77/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 23.4321\n",
      "Epoch 78/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 23.3907\n",
      "Epoch 79/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.3470\n",
      "Epoch 80/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.3083\n",
      "Epoch 81/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.2696\n",
      "Epoch 82/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.2257\n",
      "Epoch 83/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 23.1849\n",
      "Epoch 84/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.1471\n",
      "Epoch 85/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 23.1037\n",
      "Epoch 86/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.0629\n",
      "Epoch 87/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.0233\n",
      "Epoch 88/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 22.9808\n",
      "Epoch 89/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 22.9393\n",
      "Epoch 90/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.9011\n",
      "Epoch 91/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.8609\n",
      "Epoch 92/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.8180\n",
      "Epoch 93/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 22.7780\n",
      "Epoch 94/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.7359\n",
      "Epoch 95/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 22.6978\n",
      "Epoch 96/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 22.6574\n",
      "Epoch 97/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 22.6181\n",
      "Epoch 98/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 22.5789\n",
      "Epoch 99/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 22.5356\n",
      "Epoch 100/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.4948\n",
      "Epoch 101/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.4561\n",
      "Epoch 102/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.4158\n",
      "Epoch 103/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.3736\n",
      "Epoch 104/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.3356\n",
      "Epoch 105/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.2941\n",
      "Epoch 106/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.2573\n",
      "Epoch 107/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.2180\n",
      "Epoch 108/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 22.1744\n",
      "Epoch 109/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.1316\n",
      "Epoch 110/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.0944\n",
      "Epoch 111/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.0537\n",
      "Epoch 112/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.0152\n",
      "Epoch 113/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.9742\n",
      "Epoch 114/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.9350\n",
      "Epoch 115/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.8938\n",
      "Epoch 116/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.8535\n",
      "Epoch 117/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.8132\n",
      "Epoch 118/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.7765\n",
      "Epoch 119/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.7373\n",
      "Epoch 120/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.6994\n",
      "Epoch 121/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.6579\n",
      "Epoch 122/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.6163\n",
      "Epoch 123/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.5756\n",
      "Epoch 124/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.5340\n",
      "Epoch 125/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.4929\n",
      "Epoch 126/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.4559\n",
      "Epoch 127/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.4145\n",
      "Epoch 128/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.3741\n",
      "Epoch 129/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.3343\n",
      "Epoch 130/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.2984\n",
      "Epoch 131/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.2590\n",
      "Epoch 132/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.2218\n",
      "Epoch 133/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.1828\n",
      "Epoch 134/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.1463\n",
      "Epoch 135/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.1068\n",
      "Epoch 136/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.0650\n",
      "Epoch 137/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 21.0235\n",
      "Epoch 138/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.9868\n",
      "Epoch 139/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.9476\n",
      "Epoch 140/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 20.9102\n",
      "Epoch 141/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.8724\n",
      "Epoch 142/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.8317\n",
      "Epoch 143/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.7933\n",
      "Epoch 144/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 20.7550\n",
      "Epoch 145/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.7179\n",
      "Epoch 146/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.6768\n",
      "Epoch 147/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 20.6372\n",
      "Epoch 148/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 20.5975\n",
      "Epoch 149/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 20.5586\n",
      "Epoch 150/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.5201\n",
      "Epoch 151/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 20.4864\n",
      "Epoch 152/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.4451\n",
      "Epoch 153/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.4065\n",
      "Epoch 154/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.3708\n",
      "Epoch 155/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.3300\n",
      "Epoch 156/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 20.2949\n",
      "Epoch 157/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.2552\n",
      "Epoch 158/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.2165\n",
      "Epoch 159/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.1794\n",
      "Epoch 160/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.1416\n",
      "Epoch 161/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.1060\n",
      "Epoch 162/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.0646\n",
      "Epoch 163/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 20.0279\n",
      "Epoch 164/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.9912\n",
      "Epoch 165/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.9541\n",
      "Epoch 166/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.9169\n",
      "Epoch 167/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.8780\n",
      "Epoch 168/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.8367\n",
      "Epoch 169/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.8024\n",
      "Epoch 170/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.7656\n",
      "Epoch 171/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.7309\n",
      "Epoch 172/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.6908\n",
      "Epoch 173/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.6533\n",
      "Epoch 174/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.6178\n",
      "Epoch 175/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.5817\n",
      "Epoch 176/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.5430\n",
      "Epoch 177/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.5078\n",
      "Epoch 178/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.4690\n",
      "Epoch 179/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.4343\n",
      "Epoch 180/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.3994\n",
      "Epoch 181/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.3643\n",
      "Epoch 182/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.3293\n",
      "Epoch 183/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.2943\n",
      "Epoch 184/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.2580\n",
      "Epoch 185/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.2171\n",
      "Epoch 186/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.1787\n",
      "Epoch 187/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.1414\n",
      "Epoch 188/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.1044\n",
      "Epoch 189/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.0714\n",
      "Epoch 190/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 19.0348\n",
      "Epoch 191/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.9953\n",
      "Epoch 192/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.9584\n",
      "Epoch 193/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.9200\n",
      "Epoch 194/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.8827\n",
      "Epoch 195/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.8463\n",
      "Epoch 196/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.8086\n",
      "Epoch 197/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.7726\n",
      "Epoch 198/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.7341\n",
      "Epoch 199/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.6968\n",
      "Epoch 200/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.6599\n",
      "Epoch 201/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 18.6237\n",
      "Epoch 202/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.5882\n",
      "Epoch 203/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.5502\n",
      "Epoch 204/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.5115\n",
      "Epoch 205/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.4723\n",
      "Epoch 206/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.4379\n",
      "Epoch 207/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.4013\n",
      "Epoch 208/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.3648\n",
      "Epoch 209/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.3275\n",
      "Epoch 210/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.2937\n",
      "Epoch 211/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.2573\n",
      "Epoch 212/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.2191\n",
      "Epoch 213/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.1857\n",
      "Epoch 214/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.1507\n",
      "Epoch 215/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.1129\n",
      "Epoch 216/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.0757\n",
      "Epoch 217/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.0420\n",
      "Epoch 218/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.0037\n",
      "Epoch 219/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.9682\n",
      "Epoch 220/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.9319\n",
      "Epoch 221/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.8963\n",
      "Epoch 222/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.8615\n",
      "Epoch 223/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.8267\n",
      "Epoch 224/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 17.7882\n",
      "Epoch 225/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 17.7507\n",
      "Epoch 226/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.7176\n",
      "Epoch 227/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.6839\n",
      "Epoch 228/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.6463\n",
      "Epoch 229/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.6139\n",
      "Epoch 230/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.5818\n",
      "Epoch 231/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.5453\n",
      "Epoch 232/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.5089\n",
      "Epoch 233/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.4736\n",
      "Epoch 234/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 17.4379\n",
      "Epoch 235/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 17.3999\n",
      "Epoch 236/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.3657\n",
      "Epoch 237/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.3310\n",
      "Epoch 238/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.2982\n",
      "Epoch 239/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.2610\n",
      "Epoch 240/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.2272\n",
      "Epoch 241/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 17.1906\n",
      "Epoch 242/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.1590\n",
      "Epoch 243/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.1250\n",
      "Epoch 244/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.0883\n",
      "Epoch 245/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.0537\n",
      "Epoch 246/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 17.0177\n",
      "Epoch 247/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.9834\n",
      "Epoch 248/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.9486\n",
      "Epoch 249/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 16.9137\n",
      "Epoch 250/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 16.8808\n",
      "Epoch 251/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.8444\n",
      "Epoch 252/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 16.8082\n",
      "Epoch 253/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.7754\n",
      "Epoch 254/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.7418\n",
      "Epoch 255/2000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 16.7062\n",
      "Epoch 256/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 16.6703\n",
      "Epoch 257/2000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 16.6374\n",
      "Epoch 258/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 16.6049\n",
      "Epoch 259/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 16.5724\n",
      "Epoch 260/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.5388\n",
      "Epoch 261/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.5069\n",
      "Epoch 262/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 16.4717\n",
      "Epoch 263/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 16.4375\n",
      "Epoch 264/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.4068\n",
      "Epoch 265/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.3712\n",
      "Epoch 266/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.3367\n",
      "Epoch 267/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 16.3025\n",
      "Epoch 268/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.2679\n",
      "Epoch 269/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.2346\n",
      "Epoch 270/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 16.2021\n",
      "Epoch 271/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 16.1699\n",
      "Epoch 272/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 16.1382\n",
      "Epoch 273/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 16.1035\n",
      "Epoch 274/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 16.0699\n",
      "Epoch 275/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.0345\n",
      "Epoch 276/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 16.0004\n",
      "Epoch 277/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.9658\n",
      "Epoch 278/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.9318\n",
      "Epoch 279/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.8993\n",
      "Epoch 280/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.8637\n",
      "Epoch 281/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.8281\n",
      "Epoch 282/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.7945\n",
      "Epoch 283/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.7603\n",
      "Epoch 284/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.7302\n",
      "Epoch 285/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.6974\n",
      "Epoch 286/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.6650\n",
      "Epoch 287/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.6307\n",
      "Epoch 288/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.5999\n",
      "Epoch 289/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.5695\n",
      "Epoch 290/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.5383\n",
      "Epoch 291/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.5058\n",
      "Epoch 292/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 15.4723\n",
      "Epoch 293/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.4372\n",
      "Epoch 294/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 15.4036\n",
      "Epoch 295/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 15.3719\n",
      "Epoch 296/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.3367\n",
      "Epoch 297/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 15.3039\n",
      "Epoch 298/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.2723\n",
      "Epoch 299/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.2377\n",
      "Epoch 300/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.2083\n",
      "Epoch 301/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.1747\n",
      "Epoch 302/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 15.1441\n",
      "Epoch 303/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 15.1121\n",
      "Epoch 304/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.0769\n",
      "Epoch 305/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.0450\n",
      "Epoch 306/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 15.0124\n",
      "Epoch 307/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.9826\n",
      "Epoch 308/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.9480\n",
      "Epoch 309/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.9167\n",
      "Epoch 310/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.8839\n",
      "Epoch 311/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.8499\n",
      "Epoch 312/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.8160\n",
      "Epoch 313/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.7826\n",
      "Epoch 314/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.7501\n",
      "Epoch 315/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.7191\n",
      "Epoch 316/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.6866\n",
      "Epoch 317/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.6520\n",
      "Epoch 318/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.6184\n",
      "Epoch 319/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.5872\n",
      "Epoch 320/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.5556\n",
      "Epoch 321/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.5244\n",
      "Epoch 322/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.4909\n",
      "Epoch 323/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.4618\n",
      "Epoch 324/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.4290\n",
      "Epoch 325/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.3960\n",
      "Epoch 326/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.3654\n",
      "Epoch 327/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.3324\n",
      "Epoch 328/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.3006\n",
      "Epoch 329/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.2712\n",
      "Epoch 330/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.2393\n",
      "Epoch 331/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.2085\n",
      "Epoch 332/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 14.1765\n",
      "Epoch 333/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.1429\n",
      "Epoch 334/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.1087\n",
      "Epoch 335/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.0756\n",
      "Epoch 336/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 14.0463\n",
      "Epoch 337/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 14.0160\n",
      "Epoch 338/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.9846\n",
      "Epoch 339/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.9558\n",
      "Epoch 340/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.9247\n",
      "Epoch 341/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.8941\n",
      "Epoch 342/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.8642\n",
      "Epoch 343/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.8329\n",
      "Epoch 344/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 13.7997\n",
      "Epoch 345/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.7665\n",
      "Epoch 346/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.7344\n",
      "Epoch 347/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.7042\n",
      "Epoch 348/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.6714\n",
      "Epoch 349/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.6387\n",
      "Epoch 350/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.6066\n",
      "Epoch 351/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.5758\n",
      "Epoch 352/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.5452\n",
      "Epoch 353/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.5129\n",
      "Epoch 354/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.4817\n",
      "Epoch 355/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.4499\n",
      "Epoch 356/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 13.4172\n",
      "Epoch 357/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.3876\n",
      "Epoch 358/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.3546\n",
      "Epoch 359/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.3254\n",
      "Epoch 360/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 13.2949\n",
      "Epoch 361/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.2628\n",
      "Epoch 362/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.2340\n",
      "Epoch 363/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.2034\n",
      "Epoch 364/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.1722\n",
      "Epoch 365/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.1431\n",
      "Epoch 366/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.1141\n",
      "Epoch 367/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 13.0840\n",
      "Epoch 368/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.0549\n",
      "Epoch 369/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 13.0227\n",
      "Epoch 370/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.9918\n",
      "Epoch 371/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.9640\n",
      "Epoch 372/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.9346\n",
      "Epoch 373/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.9033\n",
      "Epoch 374/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.8744\n",
      "Epoch 375/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.8444\n",
      "Epoch 376/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.8127\n",
      "Epoch 377/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.7820\n",
      "Epoch 378/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.7540\n",
      "Epoch 379/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.7225\n",
      "Epoch 380/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 12.6938\n",
      "Epoch 381/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.6636\n",
      "Epoch 382/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.6341\n",
      "Epoch 383/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.6059\n",
      "Epoch 384/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 12.5770\n",
      "Epoch 385/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.5455\n",
      "Epoch 386/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.5146\n",
      "Epoch 387/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.4868\n",
      "Epoch 388/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.4574\n",
      "Epoch 389/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.4276\n",
      "Epoch 390/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.3985\n",
      "Epoch 391/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.3696\n",
      "Epoch 392/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.3379\n",
      "Epoch 393/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.3087\n",
      "Epoch 394/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.2772\n",
      "Epoch 395/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.2481\n",
      "Epoch 396/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 12.2180\n",
      "Epoch 397/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.1881\n",
      "Epoch 398/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 12.1592\n",
      "Epoch 399/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.1291\n",
      "Epoch 400/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.1001\n",
      "Epoch 401/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 12.0736\n",
      "Epoch 402/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.0432\n",
      "Epoch 403/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 12.0151\n",
      "Epoch 404/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.9870\n",
      "Epoch 405/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.9603\n",
      "Epoch 406/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.9316\n",
      "Epoch 407/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.9021\n",
      "Epoch 408/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.8721\n",
      "Epoch 409/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.8428\n",
      "Epoch 410/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.8130\n",
      "Epoch 411/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.7858\n",
      "Epoch 412/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.7567\n",
      "Epoch 413/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.7267\n",
      "Epoch 414/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.6994\n",
      "Epoch 415/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.6701\n",
      "Epoch 416/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.6405\n",
      "Epoch 417/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.6098\n",
      "Epoch 418/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.5801\n",
      "Epoch 419/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.5535\n",
      "Epoch 420/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.5256\n",
      "Epoch 421/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 11.4992\n",
      "Epoch 422/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.4703\n",
      "Epoch 423/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.4414\n",
      "Epoch 424/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.4112\n",
      "Epoch 425/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.3834\n",
      "Epoch 426/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.3540\n",
      "Epoch 427/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.3256\n",
      "Epoch 428/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.2989\n",
      "Epoch 429/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.2698\n",
      "Epoch 430/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 11.2432\n",
      "Epoch 431/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.2140\n",
      "Epoch 432/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.1851\n",
      "Epoch 433/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.1595\n",
      "Epoch 434/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 11.1300\n",
      "Epoch 435/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.1020\n",
      "Epoch 436/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.0754\n",
      "Epoch 437/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 11.0466\n",
      "Epoch 438/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 11.0203\n",
      "Epoch 439/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 10.9925\n",
      "Epoch 440/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 10.9648\n",
      "Epoch 441/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.9360\n",
      "Epoch 442/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.9066\n",
      "Epoch 443/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.8779\n",
      "Epoch 444/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 10.8508\n",
      "Epoch 445/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.8252\n",
      "Epoch 446/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.7988\n",
      "Epoch 447/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.7700\n",
      "Epoch 448/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 10.7420\n",
      "Epoch 449/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 10.7151\n",
      "Epoch 450/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.6884\n",
      "Epoch 451/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.6630\n",
      "Epoch 452/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.6347\n",
      "Epoch 453/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.6085\n",
      "Epoch 454/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.5808\n",
      "Epoch 455/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.5523\n",
      "Epoch 456/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.5262\n",
      "Epoch 457/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.4974\n",
      "Epoch 458/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.4692\n",
      "Epoch 459/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.4432\n",
      "Epoch 460/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.4154\n",
      "Epoch 461/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.3889\n",
      "Epoch 462/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.3616\n",
      "Epoch 463/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.3357\n",
      "Epoch 464/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.3090\n",
      "Epoch 465/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 10.2802\n",
      "Epoch 466/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.2532\n",
      "Epoch 467/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.2281\n",
      "Epoch 468/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.2030\n",
      "Epoch 469/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 10.1758\n",
      "Epoch 470/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.1494\n",
      "Epoch 471/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.1221\n",
      "Epoch 472/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 10.0940\n",
      "Epoch 473/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.0666\n",
      "Epoch 474/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.0405\n",
      "Epoch 475/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 10.0153\n",
      "Epoch 476/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.9892\n",
      "Epoch 477/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.9643\n",
      "Epoch 478/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.9360\n",
      "Epoch 479/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.9092\n",
      "Epoch 480/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.8829\n",
      "Epoch 481/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.8580\n",
      "Epoch 482/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.8331\n",
      "Epoch 483/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.8053\n",
      "Epoch 484/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.7781\n",
      "Epoch 485/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.7520\n",
      "Epoch 486/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.7268\n",
      "Epoch 487/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.7015\n",
      "Epoch 488/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.6740\n",
      "Epoch 489/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.6491\n",
      "Epoch 490/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.6235\n",
      "Epoch 491/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.5972\n",
      "Epoch 492/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.5730\n",
      "Epoch 493/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.5478\n",
      "Epoch 494/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.5209\n",
      "Epoch 495/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.4949\n",
      "Epoch 496/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.4680\n",
      "Epoch 497/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.4415\n",
      "Epoch 498/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.4140\n",
      "Epoch 499/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.3883\n",
      "Epoch 500/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.3638\n",
      "Epoch 501/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.3387\n",
      "Epoch 502/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.3148\n",
      "Epoch 503/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.2902\n",
      "Epoch 504/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.2635\n",
      "Epoch 505/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.2370\n",
      "Epoch 506/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.2111\n",
      "Epoch 507/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.1872\n",
      "Epoch 508/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.1613\n",
      "Epoch 509/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.1363\n",
      "Epoch 510/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.1100\n",
      "Epoch 511/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.0840\n",
      "Epoch 512/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.0579\n",
      "Epoch 513/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.0341\n",
      "Epoch 514/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.0104\n",
      "Epoch 515/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.9853\n",
      "Epoch 516/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.9590\n",
      "Epoch 517/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.9344\n",
      "Epoch 518/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.9089\n",
      "Epoch 519/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.8835\n",
      "Epoch 520/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 8.8569\n",
      "Epoch 521/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.8330\n",
      "Epoch 522/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.8081\n",
      "Epoch 523/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.7843\n",
      "Epoch 524/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.7609\n",
      "Epoch 525/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.7351\n",
      "Epoch 526/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.7102\n",
      "Epoch 527/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.6861\n",
      "Epoch 528/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.6603\n",
      "Epoch 529/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.6370\n",
      "Epoch 530/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.6109\n",
      "Epoch 531/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.5881\n",
      "Epoch 532/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.5639\n",
      "Epoch 533/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.5384\n",
      "Epoch 534/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.5146\n",
      "Epoch 535/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.4915\n",
      "Epoch 536/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.4665\n",
      "Epoch 537/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.4417\n",
      "Epoch 538/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.4187\n",
      "Epoch 539/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.3931\n",
      "Epoch 540/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 8.3685\n",
      "Epoch 541/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.3436\n",
      "Epoch 542/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.3187\n",
      "Epoch 543/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.2950\n",
      "Epoch 544/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.2723\n",
      "Epoch 545/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.2491\n",
      "Epoch 546/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.2262\n",
      "Epoch 547/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.2022\n",
      "Epoch 548/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.1770\n",
      "Epoch 549/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.1525\n",
      "Epoch 550/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.1301\n",
      "Epoch 551/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.1056\n",
      "Epoch 552/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.0805\n",
      "Epoch 553/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.0576\n",
      "Epoch 554/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.0349\n",
      "Epoch 555/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.0110\n",
      "Epoch 556/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.9881\n",
      "Epoch 557/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.9657\n",
      "Epoch 558/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.9422\n",
      "Epoch 559/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.9172\n",
      "Epoch 560/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.8924\n",
      "Epoch 561/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.8690\n",
      "Epoch 562/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.8471\n",
      "Epoch 563/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.8245\n",
      "Epoch 564/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.8008\n",
      "Epoch 565/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.7764\n",
      "Epoch 566/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.7527\n",
      "Epoch 567/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.7305\n",
      "Epoch 568/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.7090\n",
      "Epoch 569/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.6866\n",
      "Epoch 570/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.6624\n",
      "Epoch 571/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.6384\n",
      "Epoch 572/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.6159\n",
      "Epoch 573/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.5935\n",
      "Epoch 574/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.5701\n",
      "Epoch 575/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.5481\n",
      "Epoch 576/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.5252\n",
      "Epoch 577/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.5021\n",
      "Epoch 578/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.4790\n",
      "Epoch 579/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.4554\n",
      "Epoch 580/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.4339\n",
      "Epoch 581/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.4119\n",
      "Epoch 582/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.3893\n",
      "Epoch 583/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.3662\n",
      "Epoch 584/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.3422\n",
      "Epoch 585/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.3199\n",
      "Epoch 586/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.2977\n",
      "Epoch 587/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.2746\n",
      "Epoch 588/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.2512\n",
      "Epoch 589/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.2280\n",
      "Epoch 590/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.2053\n",
      "Epoch 591/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.1827\n",
      "Epoch 592/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.1602\n",
      "Epoch 593/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.1377\n",
      "Epoch 594/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 7.1154\n",
      "Epoch 595/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.0929\n",
      "Epoch 596/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.0696\n",
      "Epoch 597/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.0482\n",
      "Epoch 598/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.0276\n",
      "Epoch 599/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.0048\n",
      "Epoch 600/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.9827\n",
      "Epoch 601/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.9600\n",
      "Epoch 602/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.9389\n",
      "Epoch 603/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.9188\n",
      "Epoch 604/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.8965\n",
      "Epoch 605/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.8748\n",
      "Epoch 606/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.8523\n",
      "Epoch 607/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.8313\n",
      "Epoch 608/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.8098\n",
      "Epoch 609/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.7872\n",
      "Epoch 610/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.7666\n",
      "Epoch 611/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.7452\n",
      "Epoch 612/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.7225\n",
      "Epoch 613/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.7002\n",
      "Epoch 614/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.6801\n",
      "Epoch 615/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.6586\n",
      "Epoch 616/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.6378\n",
      "Epoch 617/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.6155\n",
      "Epoch 618/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.5939\n",
      "Epoch 619/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.5722\n",
      "Epoch 620/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.5508\n",
      "Epoch 621/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.5311\n",
      "Epoch 622/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.5096\n",
      "Epoch 623/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.4876\n",
      "Epoch 624/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.4668\n",
      "Epoch 625/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.4465\n",
      "Epoch 626/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.4260\n",
      "Epoch 627/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.4046\n",
      "Epoch 628/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.3832\n",
      "Epoch 629/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.3611\n",
      "Epoch 630/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.3399\n",
      "Epoch 631/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.3195\n",
      "Epoch 632/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.2993\n",
      "Epoch 633/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.2775\n",
      "Epoch 634/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.2559\n",
      "Epoch 635/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.2357\n",
      "Epoch 636/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.2151\n",
      "Epoch 637/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.1940\n",
      "Epoch 638/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.1739\n",
      "Epoch 639/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.1525\n",
      "Epoch 640/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.1330\n",
      "Epoch 641/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.1121\n",
      "Epoch 642/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 6.0923\n",
      "Epoch 643/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.0709\n",
      "Epoch 644/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.0498\n",
      "Epoch 645/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.0292\n",
      "Epoch 646/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.0105\n",
      "Epoch 647/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.9909\n",
      "Epoch 648/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.9704\n",
      "Epoch 649/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.9510\n",
      "Epoch 650/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.9301\n",
      "Epoch 651/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.9100\n",
      "Epoch 652/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.8889\n",
      "Epoch 653/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.8692\n",
      "Epoch 654/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.8482\n",
      "Epoch 655/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.8291\n",
      "Epoch 656/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.8093\n",
      "Epoch 657/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.7897\n",
      "Epoch 658/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.7700\n",
      "Epoch 659/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.7496\n",
      "Epoch 660/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.7291\n",
      "Epoch 661/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.7110\n",
      "Epoch 662/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6920\n",
      "Epoch 663/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6734\n",
      "Epoch 664/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6534\n",
      "Epoch 665/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6336\n",
      "Epoch 666/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.6141\n",
      "Epoch 667/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.5937\n",
      "Epoch 668/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.5753\n",
      "Epoch 669/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.5550\n",
      "Epoch 670/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.5353\n",
      "Epoch 671/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.5156\n",
      "Epoch 672/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.4971\n",
      "Epoch 673/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.4779\n",
      "Epoch 674/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 5.4582\n",
      "Epoch 675/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.4396\n",
      "Epoch 676/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.4198\n",
      "Epoch 677/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.4007\n",
      "Epoch 678/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.3818\n",
      "Epoch 679/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.3635\n",
      "Epoch 680/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.3450\n",
      "Epoch 681/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.3254\n",
      "Epoch 682/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.3053\n",
      "Epoch 683/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.2858\n",
      "Epoch 684/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.2674\n",
      "Epoch 685/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.2491\n",
      "Epoch 686/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.2316\n",
      "Epoch 687/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.2127\n",
      "Epoch 688/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.1943\n",
      "Epoch 689/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.1759\n",
      "Epoch 690/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.1575\n",
      "Epoch 691/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.1390\n",
      "Epoch 692/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.1207\n",
      "Epoch 693/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.1028\n",
      "Epoch 694/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 5.0836\n",
      "Epoch 695/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.0645\n",
      "Epoch 696/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.0470\n",
      "Epoch 697/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.0284\n",
      "Epoch 698/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.0095\n",
      "Epoch 699/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9903\n",
      "Epoch 700/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9712\n",
      "Epoch 701/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9529\n",
      "Epoch 702/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9347\n",
      "Epoch 703/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.9167\n",
      "Epoch 704/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4.8978\n",
      "Epoch 705/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.8794\n",
      "Epoch 706/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.8619\n",
      "Epoch 707/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.8438\n",
      "Epoch 708/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.8261\n",
      "Epoch 709/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.8069\n",
      "Epoch 710/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.7890\n",
      "Epoch 711/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.7706\n",
      "Epoch 712/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.7529\n",
      "Epoch 713/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.7348\n",
      "Epoch 714/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.7163\n",
      "Epoch 715/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.6989\n",
      "Epoch 716/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.6812\n",
      "Epoch 717/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.6635\n",
      "Epoch 718/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.6451\n",
      "Epoch 719/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.6282\n",
      "Epoch 720/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.6116\n",
      "Epoch 721/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.5936\n",
      "Epoch 722/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.5763\n",
      "Epoch 723/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.5592\n",
      "Epoch 724/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.5419\n",
      "Epoch 725/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.5239\n",
      "Epoch 726/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.5073\n",
      "Epoch 727/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.4901\n",
      "Epoch 728/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.4721\n",
      "Epoch 729/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.4556\n",
      "Epoch 730/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.4379\n",
      "Epoch 731/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.4207\n",
      "Epoch 732/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 4.4042\n",
      "Epoch 733/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.3879\n",
      "Epoch 734/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.3701\n",
      "Epoch 735/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.3528\n",
      "Epoch 736/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.3351\n",
      "Epoch 737/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.3178\n",
      "Epoch 738/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.3018\n",
      "Epoch 739/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.2858\n",
      "Epoch 740/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.2699\n",
      "Epoch 741/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.2535\n",
      "Epoch 742/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.2371\n",
      "Epoch 743/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.2209\n",
      "Epoch 744/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.2049\n",
      "Epoch 745/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.1876\n",
      "Epoch 746/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.1707\n",
      "Epoch 747/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.1542\n",
      "Epoch 748/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.1370\n",
      "Epoch 749/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.1205\n",
      "Epoch 750/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.1042\n",
      "Epoch 751/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0883\n",
      "Epoch 752/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0707\n",
      "Epoch 753/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0539\n",
      "Epoch 754/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0382\n",
      "Epoch 755/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.0211\n",
      "Epoch 756/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 4.0055\n",
      "Epoch 757/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9895\n",
      "Epoch 758/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9738\n",
      "Epoch 759/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9569\n",
      "Epoch 760/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9407\n",
      "Epoch 761/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9243\n",
      "Epoch 762/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9077\n",
      "Epoch 763/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8920\n",
      "Epoch 764/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8757\n",
      "Epoch 765/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8595\n",
      "Epoch 766/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8440\n",
      "Epoch 767/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8272\n",
      "Epoch 768/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.8119\n",
      "Epoch 769/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7955\n",
      "Epoch 770/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7796\n",
      "Epoch 771/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.7639\n",
      "Epoch 772/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7476\n",
      "Epoch 773/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7323\n",
      "Epoch 774/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7164\n",
      "Epoch 775/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7004\n",
      "Epoch 776/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.6846\n",
      "Epoch 777/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6694\n",
      "Epoch 778/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.6541\n",
      "Epoch 779/2000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 3.6388\n",
      "Epoch 780/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.6240\n",
      "Epoch 781/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 3.6088\n",
      "Epoch 782/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5940\n",
      "Epoch 783/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.5795\n",
      "Epoch 784/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5641\n",
      "Epoch 785/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.5482\n",
      "Epoch 786/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5331\n",
      "Epoch 787/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.5177\n",
      "Epoch 788/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.5026\n",
      "Epoch 789/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4878\n",
      "Epoch 790/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4722\n",
      "Epoch 791/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4564\n",
      "Epoch 792/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4416\n",
      "Epoch 793/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4258\n",
      "Epoch 794/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4104\n",
      "Epoch 795/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.3958\n",
      "Epoch 796/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.3812\n",
      "Epoch 797/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.3655\n",
      "Epoch 798/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.3504\n",
      "Epoch 799/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.3359\n",
      "Epoch 800/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3210\n",
      "Epoch 801/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.3063\n",
      "Epoch 802/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2920\n",
      "Epoch 803/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2775\n",
      "Epoch 804/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2623\n",
      "Epoch 805/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.2479\n",
      "Epoch 806/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2335\n",
      "Epoch 807/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.2194\n",
      "Epoch 808/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.2052\n",
      "Epoch 809/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1901\n",
      "Epoch 810/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1752\n",
      "Epoch 811/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.1602\n",
      "Epoch 812/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1455\n",
      "Epoch 813/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1311\n",
      "Epoch 814/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1172\n",
      "Epoch 815/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1028\n",
      "Epoch 816/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0890\n",
      "Epoch 817/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.0748\n",
      "Epoch 818/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0605\n",
      "Epoch 819/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.0463\n",
      "Epoch 820/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.0317\n",
      "Epoch 821/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0170\n",
      "Epoch 822/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0026\n",
      "Epoch 823/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.9886\n",
      "Epoch 824/2000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.9749\n",
      "Epoch 825/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.9607\n",
      "Epoch 826/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9465\n",
      "Epoch 827/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9323\n",
      "Epoch 828/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9180\n",
      "Epoch 829/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9044\n",
      "Epoch 830/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8903\n",
      "Epoch 831/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.8763\n",
      "Epoch 832/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8626\n",
      "Epoch 833/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8492\n",
      "Epoch 834/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.8349\n",
      "Epoch 835/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8215\n",
      "Epoch 836/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.8083\n",
      "Epoch 837/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.7950\n",
      "Epoch 838/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7811\n",
      "Epoch 839/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7676\n",
      "Epoch 840/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7545\n",
      "Epoch 841/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.7408\n",
      "Epoch 842/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.7270\n",
      "Epoch 843/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.7142\n",
      "Epoch 844/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.7012\n",
      "Epoch 845/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.6879\n",
      "Epoch 846/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6752\n",
      "Epoch 847/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6618\n",
      "Epoch 848/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6485\n",
      "Epoch 849/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6357\n",
      "Epoch 850/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6221\n",
      "Epoch 851/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6086\n",
      "Epoch 852/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.5953\n",
      "Epoch 853/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.5826\n",
      "Epoch 854/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5697\n",
      "Epoch 855/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5569\n",
      "Epoch 856/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5439\n",
      "Epoch 857/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5311\n",
      "Epoch 858/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5176\n",
      "Epoch 859/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.5051\n",
      "Epoch 860/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4922\n",
      "Epoch 861/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4793\n",
      "Epoch 862/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4663\n",
      "Epoch 863/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4535\n",
      "Epoch 864/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4407\n",
      "Epoch 865/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4276\n",
      "Epoch 866/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4152\n",
      "Epoch 867/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4026\n",
      "Epoch 868/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.3900\n",
      "Epoch 869/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.3772\n",
      "Epoch 870/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3644\n",
      "Epoch 871/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3517\n",
      "Epoch 872/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.3395\n",
      "Epoch 873/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.3275\n",
      "Epoch 874/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.3148\n",
      "Epoch 875/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3031\n",
      "Epoch 876/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2906\n",
      "Epoch 877/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2786\n",
      "Epoch 878/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2667\n",
      "Epoch 879/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2546\n",
      "Epoch 880/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2420\n",
      "Epoch 881/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2303\n",
      "Epoch 882/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2184\n",
      "Epoch 883/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2061\n",
      "Epoch 884/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1938\n",
      "Epoch 885/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1821\n",
      "Epoch 886/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1703\n",
      "Epoch 887/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1581\n",
      "Epoch 888/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1467\n",
      "Epoch 889/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1345\n",
      "Epoch 890/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1232\n",
      "Epoch 891/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.1113\n",
      "Epoch 892/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0998\n",
      "Epoch 893/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0880\n",
      "Epoch 894/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0767\n",
      "Epoch 895/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0651\n",
      "Epoch 896/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0531\n",
      "Epoch 897/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0416\n",
      "Epoch 898/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0304\n",
      "Epoch 899/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0187\n",
      "Epoch 900/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0075\n",
      "Epoch 901/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.9962\n",
      "Epoch 902/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.9850\n",
      "Epoch 903/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.9733\n",
      "Epoch 904/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.9624\n",
      "Epoch 905/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.9513\n",
      "Epoch 906/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.9399\n",
      "Epoch 907/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.9287\n",
      "Epoch 908/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.9173\n",
      "Epoch 909/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.9061\n",
      "Epoch 910/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8947\n",
      "Epoch 911/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8835\n",
      "Epoch 912/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8724\n",
      "Epoch 913/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8614\n",
      "Epoch 914/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.8501\n",
      "Epoch 915/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8390\n",
      "Epoch 916/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8280\n",
      "Epoch 917/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.8167\n",
      "Epoch 918/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.8058\n",
      "Epoch 919/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.7947\n",
      "Epoch 920/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.7839\n",
      "Epoch 921/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.7733\n",
      "Epoch 922/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.7627\n",
      "Epoch 923/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.7519\n",
      "Epoch 924/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.7415\n",
      "Epoch 925/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.7306\n",
      "Epoch 926/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.7201\n",
      "Epoch 927/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.7096\n",
      "Epoch 928/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6990\n",
      "Epoch 929/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.6887\n",
      "Epoch 930/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6783\n",
      "Epoch 931/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.6681\n",
      "Epoch 932/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6576\n",
      "Epoch 933/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6469\n",
      "Epoch 934/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.6369\n",
      "Epoch 935/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.6264\n",
      "Epoch 936/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6162\n",
      "Epoch 937/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6064\n",
      "Epoch 938/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5961\n",
      "Epoch 939/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5858\n",
      "Epoch 940/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5760\n",
      "Epoch 941/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5658\n",
      "Epoch 942/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5559\n",
      "Epoch 943/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5460\n",
      "Epoch 944/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5359\n",
      "Epoch 945/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5259\n",
      "Epoch 946/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5159\n",
      "Epoch 947/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5059\n",
      "Epoch 948/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4959\n",
      "Epoch 949/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.4859\n",
      "Epoch 950/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4761\n",
      "Epoch 951/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.4666\n",
      "Epoch 952/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.4570\n",
      "Epoch 953/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4471\n",
      "Epoch 954/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4377\n",
      "Epoch 955/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4280\n",
      "Epoch 956/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.4184\n",
      "Epoch 957/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4088\n",
      "Epoch 958/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3991\n",
      "Epoch 959/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3896\n",
      "Epoch 960/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3803\n",
      "Epoch 961/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3707\n",
      "Epoch 962/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3613\n",
      "Epoch 963/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3517\n",
      "Epoch 964/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3425\n",
      "Epoch 965/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3335\n",
      "Epoch 966/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3240\n",
      "Epoch 967/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3148\n",
      "Epoch 968/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3059\n",
      "Epoch 969/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2969\n",
      "Epoch 970/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2878\n",
      "Epoch 971/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2787\n",
      "Epoch 972/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.2694\n",
      "Epoch 973/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.2604\n",
      "Epoch 974/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.2516\n",
      "Epoch 975/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2429\n",
      "Epoch 976/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2342\n",
      "Epoch 977/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2254\n",
      "Epoch 978/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2165\n",
      "Epoch 979/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2078\n",
      "Epoch 980/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.1990\n",
      "Epoch 981/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.1904\n",
      "Epoch 982/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1818\n",
      "Epoch 983/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.1730\n",
      "Epoch 984/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1642\n",
      "Epoch 985/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.1557\n",
      "Epoch 986/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.1470\n",
      "Epoch 987/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1383\n",
      "Epoch 988/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.1299\n",
      "Epoch 989/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1214\n",
      "Epoch 990/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1131\n",
      "Epoch 991/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.1047\n",
      "Epoch 992/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.0964\n",
      "Epoch 993/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.0882\n",
      "Epoch 994/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0798\n",
      "Epoch 995/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0717\n",
      "Epoch 996/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.0633\n",
      "Epoch 997/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.0551\n",
      "Epoch 998/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0471\n",
      "Epoch 999/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0388\n",
      "Epoch 1000/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0308\n",
      "Epoch 1001/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0227\n",
      "Epoch 1002/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.0147\n",
      "Epoch 1003/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.0068\n",
      "Epoch 1004/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9989\n",
      "Epoch 1005/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9910\n",
      "Epoch 1006/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9832\n",
      "Epoch 1007/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9753\n",
      "Epoch 1008/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9673\n",
      "Epoch 1009/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9595\n",
      "Epoch 1010/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9515\n",
      "Epoch 1011/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9437\n",
      "Epoch 1012/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9362\n",
      "Epoch 1013/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9284\n",
      "Epoch 1014/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9207\n",
      "Epoch 1015/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9130\n",
      "Epoch 1016/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9054\n",
      "Epoch 1017/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8978\n",
      "Epoch 1018/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8903\n",
      "Epoch 1019/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8829\n",
      "Epoch 1020/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8754\n",
      "Epoch 1021/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8678\n",
      "Epoch 1022/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8603\n",
      "Epoch 1023/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8529\n",
      "Epoch 1024/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8456\n",
      "Epoch 1025/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8383\n",
      "Epoch 1026/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8310\n",
      "Epoch 1027/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8237\n",
      "Epoch 1028/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8166\n",
      "Epoch 1029/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8094\n",
      "Epoch 1030/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8022\n",
      "Epoch 1031/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7952\n",
      "Epoch 1032/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7880\n",
      "Epoch 1033/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7811\n",
      "Epoch 1034/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7740\n",
      "Epoch 1035/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7671\n",
      "Epoch 1036/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7601\n",
      "Epoch 1037/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7531\n",
      "Epoch 1038/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7463\n",
      "Epoch 1039/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7393\n",
      "Epoch 1040/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7326\n",
      "Epoch 1041/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7258\n",
      "Epoch 1042/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7191\n",
      "Epoch 1043/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7124\n",
      "Epoch 1044/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7057\n",
      "Epoch 1045/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6991\n",
      "Epoch 1046/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6924\n",
      "Epoch 1047/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6859\n",
      "Epoch 1048/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6792\n",
      "Epoch 1049/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6727\n",
      "Epoch 1050/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6663\n",
      "Epoch 1051/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6599\n",
      "Epoch 1052/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6535\n",
      "Epoch 1053/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6472\n",
      "Epoch 1054/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6408\n",
      "Epoch 1055/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6345\n",
      "Epoch 1056/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6281\n",
      "Epoch 1057/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6218\n",
      "Epoch 1058/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6156\n",
      "Epoch 1059/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6094\n",
      "Epoch 1060/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6033\n",
      "Epoch 1061/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5971\n",
      "Epoch 1062/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5909\n",
      "Epoch 1063/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5848\n",
      "Epoch 1064/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5787\n",
      "Epoch 1065/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5726\n",
      "Epoch 1066/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5667\n",
      "Epoch 1067/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5607\n",
      "Epoch 1068/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5548\n",
      "Epoch 1069/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5489\n",
      "Epoch 1070/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5430\n",
      "Epoch 1071/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5372\n",
      "Epoch 1072/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5313\n",
      "Epoch 1073/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5256\n",
      "Epoch 1074/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5199\n",
      "Epoch 1075/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5142\n",
      "Epoch 1076/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5086\n",
      "Epoch 1077/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5030\n",
      "Epoch 1078/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4974\n",
      "Epoch 1079/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4918\n",
      "Epoch 1080/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4862\n",
      "Epoch 1081/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4808\n",
      "Epoch 1082/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4753\n",
      "Epoch 1083/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4699\n",
      "Epoch 1084/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4644\n",
      "Epoch 1085/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4591\n",
      "Epoch 1086/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4537\n",
      "Epoch 1087/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4486\n",
      "Epoch 1088/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4432\n",
      "Epoch 1089/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4379\n",
      "Epoch 1090/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4327\n",
      "Epoch 1091/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4276\n",
      "Epoch 1092/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4223\n",
      "Epoch 1093/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4172\n",
      "Epoch 1094/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4122\n",
      "Epoch 1095/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4071\n",
      "Epoch 1096/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4020\n",
      "Epoch 1097/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3970\n",
      "Epoch 1098/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3920\n",
      "Epoch 1099/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3871\n",
      "Epoch 1100/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3822\n",
      "Epoch 1101/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3774\n",
      "Epoch 1102/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3726\n",
      "Epoch 1103/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3678\n",
      "Epoch 1104/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3631\n",
      "Epoch 1105/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3584\n",
      "Epoch 1106/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3536\n",
      "Epoch 1107/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3489\n",
      "Epoch 1108/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3443\n",
      "Epoch 1109/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3398\n",
      "Epoch 1110/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3353\n",
      "Epoch 1111/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3307\n",
      "Epoch 1112/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3261\n",
      "Epoch 1113/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3217\n",
      "Epoch 1114/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3172\n",
      "Epoch 1115/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3128\n",
      "Epoch 1116/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3083\n",
      "Epoch 1117/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3040\n",
      "Epoch 1118/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2997\n",
      "Epoch 1119/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2954\n",
      "Epoch 1120/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2911\n",
      "Epoch 1121/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2869\n",
      "Epoch 1122/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2826\n",
      "Epoch 1123/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2785\n",
      "Epoch 1124/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2743\n",
      "Epoch 1125/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2703\n",
      "Epoch 1126/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2662\n",
      "Epoch 1127/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2621\n",
      "Epoch 1128/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2580\n",
      "Epoch 1129/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2540\n",
      "Epoch 1130/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1131/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2462\n",
      "Epoch 1132/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2423\n",
      "Epoch 1133/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2385\n",
      "Epoch 1134/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2346\n",
      "Epoch 1135/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2309\n",
      "Epoch 1136/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2271\n",
      "Epoch 1137/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2233\n",
      "Epoch 1138/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2196\n",
      "Epoch 1139/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2159\n",
      "Epoch 1140/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2123\n",
      "Epoch 1141/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2087\n",
      "Epoch 1142/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2051\n",
      "Epoch 1143/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2016\n",
      "Epoch 1144/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1980\n",
      "Epoch 1145/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1946\n",
      "Epoch 1146/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1911\n",
      "Epoch 1147/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1877\n",
      "Epoch 1148/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1844\n",
      "Epoch 1149/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1810\n",
      "Epoch 1150/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1777\n",
      "Epoch 1151/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1744\n",
      "Epoch 1152/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1712\n",
      "Epoch 1153/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1679\n",
      "Epoch 1154/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1647\n",
      "Epoch 1155/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1615\n",
      "Epoch 1156/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1584\n",
      "Epoch 1157/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1553\n",
      "Epoch 1158/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1522\n",
      "Epoch 1159/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1492\n",
      "Epoch 1160/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1462\n",
      "Epoch 1161/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1432\n",
      "Epoch 1162/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1403\n",
      "Epoch 1163/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1374\n",
      "Epoch 1164/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1345\n",
      "Epoch 1165/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1316\n",
      "Epoch 1166/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1288\n",
      "Epoch 1167/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1260\n",
      "Epoch 1168/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1232\n",
      "Epoch 1169/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1205\n",
      "Epoch 1170/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1178\n",
      "Epoch 1171/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1152\n",
      "Epoch 1172/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1125\n",
      "Epoch 1173/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1100\n",
      "Epoch 1174/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1074\n",
      "Epoch 1175/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1048\n",
      "Epoch 1176/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1023\n",
      "Epoch 1177/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0999\n",
      "Epoch 1178/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0974\n",
      "Epoch 1179/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0950\n",
      "Epoch 1180/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0926\n",
      "Epoch 1181/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0903\n",
      "Epoch 1182/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0879\n",
      "Epoch 1183/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0857\n",
      "Epoch 1184/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0834\n",
      "Epoch 1185/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0812\n",
      "Epoch 1186/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0790\n",
      "Epoch 1187/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0768\n",
      "Epoch 1188/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0747\n",
      "Epoch 1189/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0726\n",
      "Epoch 1190/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0705\n",
      "Epoch 1191/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0685\n",
      "Epoch 1192/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0665\n",
      "Epoch 1193/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0645\n",
      "Epoch 1194/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0625\n",
      "Epoch 1195/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0606\n",
      "Epoch 1196/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0588\n",
      "Epoch 1197/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0569\n",
      "Epoch 1198/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0551\n",
      "Epoch 1199/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0533\n",
      "Epoch 1200/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0515\n",
      "Epoch 1201/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0498\n",
      "Epoch 1202/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0481\n",
      "Epoch 1203/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0464\n",
      "Epoch 1204/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0447\n",
      "Epoch 1205/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0431\n",
      "Epoch 1206/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0416\n",
      "Epoch 1207/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0401\n",
      "Epoch 1208/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0385\n",
      "Epoch 1209/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0370\n",
      "Epoch 1210/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0356\n",
      "Epoch 1211/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0341\n",
      "Epoch 1212/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0327\n",
      "Epoch 1213/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 1214/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0300\n",
      "Epoch 1215/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0287\n",
      "Epoch 1216/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0275\n",
      "Epoch 1217/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0262\n",
      "Epoch 1218/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0250\n",
      "Epoch 1219/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0238\n",
      "Epoch 1220/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0227\n",
      "Epoch 1221/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0215\n",
      "Epoch 1222/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0204\n",
      "Epoch 1223/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0194\n",
      "Epoch 1224/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 1225/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 1226/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0164\n",
      "Epoch 1227/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 1228/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0145\n",
      "Epoch 1229/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 1230/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 1231/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0120\n",
      "Epoch 1232/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 1233/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0104\n",
      "Epoch 1234/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 1235/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0090\n",
      "Epoch 1236/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0083\n",
      "Epoch 1237/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0076\n",
      "Epoch 1238/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0070\n",
      "Epoch 1239/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0064\n",
      "Epoch 1240/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 1241/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0053\n",
      "Epoch 1242/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0048\n",
      "Epoch 1243/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0043\n",
      "Epoch 1244/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0039\n",
      "Epoch 1245/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0034\n",
      "Epoch 1246/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0031\n",
      "Epoch 1247/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0027\n",
      "Epoch 1248/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0023\n",
      "Epoch 1249/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0020\n",
      "Epoch 1250/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0017\n",
      "Epoch 1251/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0015\n",
      "Epoch 1252/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0012\n",
      "Epoch 1253/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0010\n",
      "Epoch 1254/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.1830e-04\n",
      "Epoch 1255/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 6.5059e-04\n",
      "Epoch 1256/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 5.0784e-04\n",
      "Epoch 1257/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.8424e-04\n",
      "Epoch 1258/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.8240e-04\n",
      "Epoch 1259/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.0038e-04\n",
      "Epoch 1260/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3458e-04\n",
      "Epoch 1261/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.5362e-05\n",
      "Epoch 1262/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.0252e-05\n",
      "Epoch 1263/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6922e-05\n",
      "Epoch 1264/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2942e-05\n",
      "Epoch 1265/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 5.1732e-06\n",
      "Epoch 1266/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6987e-06\n",
      "Epoch 1267/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.3527e-07\n",
      "Epoch 1268/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.5669e-08\n",
      "Epoch 1269/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.2919e-09\n",
      "Epoch 1270/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.1283e-10\n",
      "Epoch 1271/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.3608e-11\n",
      "Epoch 1272/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2325e-13\n",
      "Epoch 1273/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1274/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1275/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1276/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1277/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1278/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1279/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1280/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1281/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1282/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1283/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1284/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1285/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1286/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1287/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1288/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1289/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1290/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1291/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1292/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1293/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1294/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1295/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1296/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1297/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1298/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1299/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1300/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1301/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1302/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1303/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1304/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1305/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1306/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1307/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1308/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1309/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1310/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1311/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1312/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1313/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1314/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1315/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1316/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1317/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1318/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1319/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1320/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1321/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1322/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1323/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1324/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1325/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1326/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1327/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1328/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1329/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1330/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1331/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1332/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1333/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1334/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1335/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1336/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1337/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1338/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1339/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1340/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1341/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1342/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1343/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1344/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1345/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1346/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1347/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1348/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1349/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1350/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1351/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1352/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1353/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1354/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1355/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1356/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1357/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1358/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1359/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1360/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1361/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1362/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1363/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1364/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1365/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1366/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1367/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1368/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1369/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1370/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1371/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1372/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1373/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1374/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1375/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1376/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1377/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1378/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1379/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1380/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1381/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1382/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1383/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1384/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1385/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1386/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1387/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1388/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1389/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1390/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1391/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1392/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1393/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1394/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1395/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1396/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1397/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1398/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1399/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1400/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1401/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1402/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1403/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1404/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1405/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1406/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1407/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1408/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1409/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1410/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1411/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1412/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1413/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1414/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1415/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1416/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1417/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1418/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1419/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1420/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1421/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1422/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1423/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1424/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1425/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1426/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1427/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1428/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1429/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1430/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1431/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1432/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1433/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1434/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1435/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1436/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1437/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1438/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1439/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1440/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1441/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1442/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1443/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1444/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1445/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1446/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1447/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1448/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1449/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1450/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1451/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1452/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1453/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1454/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1455/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1456/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1457/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1458/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1459/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1460/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1461/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1462/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1463/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1464/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1465/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1466/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1467/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1468/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1469/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1470/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1471/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1472/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1473/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1474/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1475/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1476/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1477/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1478/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1479/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1480/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1481/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1482/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1483/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1484/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1485/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1486/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1487/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1488/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1489/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1490/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1491/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1492/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1493/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1494/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1495/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1496/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1497/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1498/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1499/2000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 1500/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0000e+00\n",
      "Epoch 1501/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1502/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1503/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1504/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1505/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1506/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1507/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1508/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1509/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1510/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1511/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1512/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1513/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1514/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1515/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1516/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1517/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1518/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1519/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1520/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1521/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1522/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1523/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1524/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1525/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1526/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0000e+00\n",
      "Epoch 1527/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1528/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1529/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1530/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1531/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1532/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1533/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1534/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1535/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1536/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1537/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1538/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1539/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1540/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1541/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1542/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1543/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1544/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1545/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1546/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1547/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1548/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1549/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1550/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1551/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1552/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1553/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1554/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1555/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1556/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1557/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1558/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1559/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1560/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1561/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1562/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1563/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1564/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1565/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1566/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1567/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1568/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1569/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1570/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1571/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1572/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1573/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1574/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1575/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1576/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1577/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1578/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1579/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1580/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1581/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1582/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1583/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1584/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1585/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1586/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1587/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1588/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1589/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1590/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1591/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1592/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1593/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1594/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1595/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1596/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1597/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1598/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1599/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1600/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1601/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1602/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1603/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1604/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1605/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1606/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1607/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1608/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1609/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1610/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1611/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1612/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1613/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1614/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1615/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1616/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1617/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1618/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1619/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1620/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1621/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1622/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1623/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1624/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1625/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1626/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1627/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1628/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1629/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1630/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1631/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1632/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1633/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1634/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1635/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1636/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1637/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1638/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1639/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1640/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1641/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1642/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1643/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1644/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1645/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1646/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1647/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1648/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1649/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1650/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1651/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1652/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1653/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1654/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1655/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1656/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1657/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1658/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1659/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1660/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1661/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1662/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1663/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1664/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1665/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1666/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1667/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1668/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1669/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1670/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1671/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1672/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1673/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1674/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1675/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1676/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1677/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1678/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1679/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1680/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1681/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1682/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1683/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1684/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1685/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1686/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1687/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1688/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1689/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1690/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1691/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1692/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1693/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1694/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1695/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1696/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1697/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1698/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1699/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1700/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1701/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1702/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1703/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1704/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1705/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1706/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1707/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1708/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1709/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1710/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1711/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1712/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1713/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1714/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1715/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1716/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1717/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1718/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1719/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1720/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1721/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1722/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1723/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1724/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1725/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1726/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1727/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1728/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1729/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1730/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1731/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1732/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1733/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1734/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1735/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1736/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1737/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1738/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1739/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1740/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1741/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1742/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1743/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1744/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1745/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1746/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1747/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1748/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1749/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1750/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1751/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1752/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1753/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1754/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1755/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1756/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1757/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1758/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1759/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1760/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1761/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1762/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1763/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1764/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1765/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1766/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1767/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1768/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1769/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1770/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1771/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1772/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1773/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1774/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1775/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1776/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1777/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1778/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1779/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1780/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1781/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1782/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1783/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1784/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1785/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1786/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1787/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1788/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1789/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1790/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1791/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1792/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1793/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1794/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1795/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1796/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1797/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1798/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1799/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1800/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1801/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1802/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1803/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1804/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1805/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1806/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1807/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1808/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1809/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1810/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1811/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1812/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1813/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1814/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1815/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1816/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1817/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1818/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1819/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1820/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1821/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1822/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1823/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1824/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1825/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1826/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1827/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1828/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1829/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1830/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1831/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1832/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1833/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1834/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1835/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1836/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1837/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1838/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1839/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1840/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1841/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1842/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1843/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1844/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1845/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1846/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1847/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1848/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1849/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1850/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1851/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1852/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1853/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1854/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1855/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1856/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1857/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1858/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1859/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1860/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1861/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1862/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1863/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1864/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1865/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1866/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1867/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1868/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1869/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1870/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1871/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1872/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1873/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1874/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1875/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1876/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1877/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1878/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1879/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1880/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1881/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1882/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1883/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1884/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1885/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1886/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1887/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1888/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1889/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1890/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1891/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1892/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1893/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1894/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1895/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1896/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1897/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1898/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1899/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1900/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1901/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1902/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1903/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1904/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1905/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1906/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1907/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1908/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1909/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1910/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1911/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1912/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1913/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1914/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1915/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1916/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1917/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1918/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1919/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1920/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1921/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1922/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1923/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1924/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1925/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1926/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1927/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1928/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1929/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1930/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1931/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1932/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1933/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1934/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1935/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1936/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1937/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1938/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1939/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1940/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1941/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1942/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1943/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1944/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1945/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1946/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1947/2000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.0000e+00\n",
      "Epoch 1948/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1949/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0000e+00\n",
      "Epoch 1950/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1951/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1952/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1953/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1954/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1955/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1956/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1957/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1958/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1959/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1960/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1961/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1962/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1963/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1964/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1965/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1966/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1967/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1968/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1969/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1970/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1971/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1972/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1973/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1974/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1975/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1976/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1977/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1978/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1979/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0000e+00\n",
      "Epoch 1980/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1981/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1982/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1983/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1984/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1985/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1986/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1987/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1988/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1989/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1990/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1991/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1992/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1993/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1994/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1995/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1996/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Epoch 1997/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1998/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 1999/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
      "Epoch 2000/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "x_tr = tf.cast(tf.linspace(-1, 1, 101), dtype=tf.float32)\n",
    "y_tr = x_tr * 2 + 5\n",
    "\n",
    "################################\n",
    "# Insertar código.\n",
    "history = model.fit(x_tr, y_tr, epochs=2000)\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights: [array([[2.]], dtype=float32), array([5.], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Optimal weights:\", model.get_weights())\n",
    "\n",
    "plt.plot(x_tr.numpy(), y_tr.numpy(), \"*\", label=\"Observed\")\n",
    "plt.plot(x_tr.numpy(), model.predict(x_tr, verbose=0), \"-\", label=\"Model\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción a las capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tradicionalmente, las ANN se organiza en capas de unidades.\n",
    "    * La salida de las neuronas de una capa se usa como entrada de la capa siguiente.\n",
    "\n",
    "<img src=\"figures/MLP.svg?15\" alt=\"MLP clásico.\">\n",
    "\n",
    "* En el contexto de TensorFlow, las capas se generalizan a funciones con una cierta estructura matemática conocida, que se pueden reutilizar y que tienen variables que pueden ser entrenadas.\n",
    "* Intuitivamente, se pueden considerar como las piezas fundamentales de un modelo de ML.\n",
    "    * Realizan una cierta transformación desde los datos de entrada a los datos de salida.\n",
    "    * Esta transformación se puede adaptar a los datos durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principales tipos de capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En Keras existen diversos tipos de capas:\n",
    "    * Capas básicas.\n",
    "    * Capa de convolución.\n",
    "    * Capas de *pooling*.\n",
    "    * Capas recurrentes.\n",
    "    * Capas de preproceso.\n",
    "    * Capas de normalización.\n",
    "    * Capas de regularización.\n",
    "    * Capas de cambio de forma.\n",
    "    * Capas de combinación.\n",
    "* En la siguiente tabla se muestran algunas de las principales capas básicas:\n",
    "\n",
    "| Tipo de Capa | Clase | Descripción                                                                          |\n",
    "|:-------------------|:------------------------|:-------------------------------------------------------------|\n",
    "| Capa de entrada    | `keras.Input`           | Capa que tomará  los valores de la entrada.                  |\n",
    "| Capa densa         | `keras.layers.Dense`    | Capa completamente conectada.                                |\n",
    "| Capa de activación| `keras.layers.Activation`| Capa para aplicar la función de activación a la salida de la capa anterior.|\n",
    "| Capa de *embedding*| `keras.layers.Embedding`| Capa para codificar valores discretos en un espacio continuo.|\n",
    "| Capa lambda     | `keras.layers.Lambda`     | Capa para aplicar una función arbitraria de TensorFlow.        |\n",
    "\n",
    "* Las capas no son más que funciones de TensorFlow, que se pueden aplicar a tensores que tengan las dimensiones apropiadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.54416585  1.3484554  -2.058876  ]]\n"
     ]
    }
   ],
   "source": [
    "layer = keras.layers.Dense(3, input_shape=(3,))\n",
    "print(layer(tf.constant([[1, 1, 1]])).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"info\"><a href=\"https://keras.io/api/layers/\">Capas de Keras</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo secuencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los modelos secuenciales de Keras están concebidos como una pila de capas, donde cada capa toma como entrada un tensor, y produce como salida otro tensor.\n",
    "* Al crear el modelo secuencial, se especifica una lista de las capas que compondrán el modelo.\n",
    "* El modelo completo actúa como una función de TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((3, 3))\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(2, activation=\"relu\", name=\"layer1\"),\n",
    "        keras.layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n",
    "        keras.layers.Dense(4, name=\"layer3\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El modelo secuencial simplemente indica que cada capa se aplica sobre la salida de la anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = keras.layers.Dense(2, activation=\"relu\", name=\"layer1\")\n",
    "layer2 = keras.layers.Dense(3, activation=\"relu\", name=\"layer2\")\n",
    "layer3 = keras.layers.Dense(4, name=\"layer3\")\n",
    "\n",
    "print(layer3(layer2(layer1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El modelo tiene un atributo `layers` con las capas que lo componen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.layers:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se puede extraer un resumen del modelo, que muestra información útil como los parámetros entrenables que tiene.\n",
    "* También se puede representar en forma de grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El modelo secuencial también se puede definir de forma incremental, añadiendo las capas sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(2, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(3, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(4))\n",
    "\n",
    "print(model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Por lo general, al crear una capa los pesos no se inicializan.\n",
    "* La primera vez que se aplica la capa se crean los pesos, con la dimensión adecuada para esa entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(3)\n",
    "print(\"Weights (before using the layer):\", layer.weights)\n",
    "\n",
    "y = layer(x)\n",
    "print(\"Weights (after using the layer): \", layer.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En el caso de modelos secuenciales, ocurre algo similar: los pesos no se inicializan hasta que no se aplica el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(2),\n",
    "        keras.layers.Dense(3),\n",
    "        keras.layers.Dense(4),\n",
    "    ]\n",
    ")\n",
    "try:\n",
    "    model.summary()\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)\n",
    "\n",
    "print(\"\\nApplying the model...\")\n",
    "model(x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sin embargo, se puede forzar a que se creen especificando la dimensión inicial con una capa de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(3,)),\n",
    "        keras.layers.Dense(2),\n",
    "        keras.layers.Dense(3),\n",
    "        keras.layers.Dense(4),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* También se puede especificar el tamaño de la entrada como un parámetro de la primera capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(2, input_shape=(3,)),\n",
    "        keras.layers.Dense(3),\n",
    "        keras.layers.Dense(4),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Definir una red neuronal con $5$ capas densas de $10$ neuronas cada una, para un problema de $20$ dimensiones de entrada y $1$ salida.\n",
    "La función de activación será la sigmoidal para las capas ocultas, y la lineal para la capa de salida.\n",
    "* Observar el número de parámetros entrenables, y comprobar que concuerda con el teórico.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Insertar código.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(10, input_shape=(20,), activation=\"sigmoid\"),\n",
    "        keras.layers.Dense(10, activation=\"sigmoid\"),\n",
    "        keras.layers.Dense(10, activation=\"sigmoid\"),\n",
    "        keras.layers.Dense(10, activation=\"sigmoid\"),\n",
    "        keras.layers.Dense(10, activation=\"sigmoid\"),\n",
    "        keras.layers.Dense(1, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "################################\n",
    "\n",
    "print(\"\\nTotal parameters (theor.):\", (20 + 1) * 10 + 4 * ((10 + 1) * 10) + (10 + 1) * 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo funcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los modelos funcionales son más flexibles que los secuenciales.\n",
    "    * Capas compartidas.\n",
    "    * Múltiples entradas/salidas.\n",
    "    * Cualquier topología de conexión.\n",
    "* El modelo se define aplicando las capas como funciones, y especificando al final cuál será la entrada del modelo y cuál la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(3,))\n",
    "x = keras.layers.Dense(2)(inputs)\n",
    "x = keras.layers.Dense(3)(x)\n",
    "outputs = keras.layers.Dense(4)(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Definir la red neuronal del ejercicio anterior usando la definición funcional.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Insertar código.\n",
    "inputs = keras.Input(shape=(20,))\n",
    "x = keras.layers.Dense(10, activation=\"sigmoid\")(inputs)\n",
    "x = keras.layers.Dense(10, activation=\"sigmoid\")(x)\n",
    "x = keras.layers.Dense(10, activation=\"sigmoid\")(x)\n",
    "x = keras.layers.Dense(10, activation=\"sigmoid\")(x)\n",
    "x = keras.layers.Dense(10, activation=\"sigmoid\")(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"linear\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ilustrar la creación, entrenamiento y predicción con un modelo de Keras, se usará el conjunto de datos de MNIST, accesible directamente desde Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_tr, y_tr), (x_te, y_te) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_tr = x_tr.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_te = x_te.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_tr = y_tr.astype(\"float32\")\n",
    "y_te = y_te.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El primer paso es crear el modelo (puede ser tanto secuencial como funcional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(784,), name=\"digits\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\", name=\"dense_1\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\", name=\"dense_2\"),\n",
    "        keras.layers.Dense(10, activation=\"softmax\", name=\"predictions\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Definir un modelo lineal de clasificación binaria, de nombre `perceptron`, para un problema de $2$ dimensiones.\n",
    "Usar la aproximación funcional.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Insertar código.\n",
    "inputs = keras.Input(shape=(2,))\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
    "perceptron = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "perceptron.summary()\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una vez creado el modelo, hay que compilarlo, especificando:\n",
    "    * La función de pérdida, que se minimizará durante el entrenamiento.\n",
    "    * El optimizador, es decir, el algoritmo que se usará para minimizar la pérdida.\n",
    "    * Las métricas con las que se evaluará el modelo para mostrar la evolución del entrenamiento.\n",
    "* Existen diversas opciones para cada uno.\n",
    "* Se pueden definir también nuevas pérdidas y métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Pérdida|Descripción   |Uso|\n",
    "|:--|:--|:--|\n",
    "|`mean_squared_error`  |Función estándar para problemas de regresión.     |Regresión|\n",
    "|`mean_squared_logarithmic_error`|Similar a la anterior, pero al ser logarítmica \"suaviza el castigo\".       |Regresión    |\n",
    "|`mean_absolute_error`|Apropiada cuando la distribución de datos tiene valores grandes o pequeños muy alejados del valor medio.|Regresión                           |\n",
    "|`binary_crossentropy` |Adecuada para problemas de clasificación binaria con valores $0$ y $1$.|Clasificación|\n",
    "|`hinge`|Pérdida de la SVM.|Clasificación|\n",
    "|`squared_hinge`|Suaviza la función de error anterior y resulta más sencilla en cómputo.|Clasificación|\n",
    "|`categorical_crossentropy`|Función de pérdida utilizado por defecto para problemas de clasificación múltiple (*hot-enconding*).|Clasificación |\n",
    "|`sparse_categorical_crossentropy`|Función de pérdida para problemas de clasificación múltiple (valores enteros).|Clasificación|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Optimizador|Velocidad|Calidad|\n",
    "|:--|:--|:--|\n",
    "|SGD|Baja|Alta|\n",
    "|Adagrad|Media|Alta|\n",
    "|RMSprop|Alta|Media-Alta|\n",
    "|Adam|Alta|Media o Alta|\n",
    "|Nadam|Alta|Media o Alta|\n",
    "|AdaMax|Alta|Media o Alta|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),                # Equivalently, \"rmsprop\".\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),   # Equivalently, \"sparse_categorical_crossentropy\".\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Compilar el modelo `perceptron` para minimizar la entropía binaria, usando ADAM, y monitorizando la precisión (*accuracy*).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Insertar código.\n",
    "perceptron.compile(\n",
    "    optimizer=\"ADAM\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"info\"><a href=\"https://keras.io/api/losses/\">Pérdidas de Keras</a></div><br>\n",
    "<div class=\"info\"><a href=\"https://keras.io/api/optimizers/\">Optimizadores de Keras</a></div><br>\n",
    "<div class=\"info\"><a href=\"https://keras.io/api/metrics/\">Métricas de Keras</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tras compilar el modelo, se puede entrenar usando el método `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    x_tr,\n",
    "    y_tr,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    validation_split=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Entrenar el modelo `perceptron` sobre el conjunto de datos definido a continuación.\n",
    "* Razonar si el modelo entrenado será capaz de resolver este problema o no.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pcp = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_pcp = np.array([0, 1, 1, 0])\n",
    "\n",
    "################################\n",
    "# Insertar código.\n",
    "history = perceptron.fit(x_pcp, y_pcp, epochs=100)\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una vez entrenado, se puede predecir con el modelo mediante el método `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test error, test accuracy:\", model.evaluate(x_te, y_te, verbose=0))\n",
    "\n",
    "pred = model.predict(x_te, verbose=0)\n",
    "print(pred.shape)\n",
    "print(pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El entrenamiento se puede realizar usando un `Dataset` de Keras, más eficiente y con opciones adicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tr = tf.data.Dataset.from_tensor_slices((x_tr, y_tr))\n",
    "dataset_tr = dataset_tr.shuffle(buffer_size=1024).batch(64)\n",
    "print(dataset_tr)\n",
    "\n",
    "model.fit(dataset_tr, epochs=3)\n",
    "model.predict(x_te, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Obtener las predicciones del modelo `perceptron` sobre el conjunto de entrenamiento `x_pcp`.\n",
    "* Analizar el resultado obtenido.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Insertar código.\n",
    "print(perceptron.predict(x_pcp, verbose=0))\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se puede almacenar un modelo con el método `save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para cargarlo basta con usar `keras.models.load_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = keras.models.load_model(\"model\")\n",
    "loaded.predict(x_te, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Almacenar el perceptrón en `\"perceptron\"`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Insertar código.\n",
    "perceptron.save(\"perceptron\")\n",
    "!ls\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opciones avanzadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para motivar algunas opciones de Keras, se utilizará el conjunto de datos Fashion MNIST.\n",
    "* Este problema consiste en distinguir entre $10$ tipos distintos de ropa:\n",
    "\n",
    "| Etiqueta| Descripción|\n",
    "|:--|:--|\n",
    "|0 |\tT-shirt/top |\n",
    "|1 |\tTrouser |\n",
    "|2 |\tPullover |\n",
    "|3 |\tDress |\n",
    "|4 |\tCoat |\n",
    "|5 |\tSandal |\n",
    "|6 |\tShirt |\n",
    "|7 |\tSneaker |\n",
    "|8 |\tBag |\n",
    "|9 |\tAnkle boot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_tr, y_tr),(x_te, y_te) = mnist.load_data()\n",
    "x_tr = x_tr\n",
    "x_te = x_te\n",
    "\n",
    "print(\"Training size:\", x_tr.shape)\n",
    "print(\"Target size:  \", y_tr.shape)\n",
    "print(\"Maximum:      \", x_tr.max())\n",
    "print(\"Minimum:      \", x_tr.min())\n",
    "\n",
    "plt.imshow(x_tr[0])\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Class {}\".format(y_tr[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambios de forma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una gran cantidad de problemas de hoy en día tienen cierta estructura multidimensional.\n",
    "    * Señales.\n",
    "    * Imágenes.\n",
    "    * Vídeos.\n",
    "* Cuando se quieren procesar este tipo de datos con capas densas, es necesario \"aplanarlos\" mediante una capa `Flatten`, que los convierte en vectores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra una arquitectura para resolver el problema Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "history = model.fit(x_tr, y_tr, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Comprobar si el resultado en test, calculado en la siguiente celda, es aceptable o no.\n",
    "* Intentar mejorarlo sin modificar la arquitectura.\n",
    "\n",
    "<div class=\"notes\">\n",
    "\n",
    "* A veces el problema no está en el modelo, sino en los datos.\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_te, y_te , verbose=0)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Insertar código.\n",
    "x_tr = x_tr / 255.\n",
    "x_te = x_te / 255.\n",
    "\n",
    "model.fit(x_tr, y_tr, epochs=5)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x_te, y_te, verbose=0)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Callbacks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los *callbacks* son una herramienta para especificar el comportamiento del modelo durante el entrenamiento, la evaluación, o la predicción.\n",
    "* Dos de los *callbacks* más importantes son:\n",
    "    * `EarlyStopping`: Permite detener el entrenamiento cuando no se produce mejora (en función de la métrica seleccionada):\n",
    "``` Python\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0,\n",
    "        patience=0,\n",
    "        verbose=0,\n",
    "        mode=\"auto\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=False,\n",
    "    )\n",
    "```\n",
    "    * `ModelCheckpoint`: Guarda periódicamente el modelo durante el entrenamiento.\n",
    "``` Python\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=0,\n",
    "        save_best_only=False,\n",
    "        save_weights_only=False,\n",
    "        mode=\"auto\",\n",
    "        save_freq=\"epoch\",\n",
    "        options=None,\n",
    "        **kwargs\n",
    "    )\n",
    "```\n",
    "* Se pueden definir *callbacks* propios, heredando de la clase `Callback`, y definiendo uno o varios de los siguientes métodos:\n",
    "| Método | Descripción |\n",
    "|:--|:--|\n",
    "|`on_(train\\|test\\|predict)_begin(self, logs=None)`| Invocada al empezar el entrenamiento/evaluación/predicción.|\n",
    "|`on_(train\\|test\\|predict)_end(self, logs=None)`| Invocada al terminar el entrenamiento/evaluación/predicción.|\n",
    "|`on_(train\\|test\\|predict)_batch_begin(self, batch, logs=None)`| Invocada al empezar a procesar un *batch* en entrenamiento/evaluación/predicción.|\n",
    "|`on_(train\\|test\\|predict)_batch_end(self, batch, logs=None)`| Invocada al terminar de procesar un *batch* en entrenamiento/evaluación/predicción.|\n",
    "|`on_epoch_begin(self, epoch, logs=None)`| Invocada al empezar una época de entrenamiento.|\n",
    "|`on_epoch_end(self, epoch, logs=None)`| Invocada al terminar una época de entrenamiento.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se define un *callback* para detener el entrenamiento si la pérdida en validación no mejora tras $3$ épocas, y otro para ir guardando el modelo (si mejora respecto al anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(patience=3)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"model\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se define un *callback* personalizado que detiene el entrenamiento si la precisión en entrenamiento supera el $60\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the model when the training accuracy arrives to 0.60.\n",
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if (logs[\"sparse_categorical_accuracy\"] > 0.6):\n",
    "            print(\"Desired accuracy achieved\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "my_stopping = MyCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, se entrena el modelo con los *callbacks* definidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_tr,\n",
    "                    y_tr,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping, checkpoint, my_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Modificar el código anterior para alcanzar una precisión mínima del $89\\%$ de entrenamiento.\n",
    "* Para evitar tiempos innecesarios de ejecución, parar el modelo si no se consigue mejorar la precisión de entrenamiento en $5$ épocas.\n",
    "* Almacenar el modelo automáticamente cuando mejore la predicción.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Insertar código.\n",
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if (logs[\"sparse_categorical_accuracy\"] > 0.89):\n",
    "            print(\"Desired accuracy achieved\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "my_stopping = MyCallback()\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=5)\n",
    "history = model.fit(x_tr,\n",
    "                    y_tr,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping, checkpoint, my_stopping])\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"info\"><a href=\"https://keras.io/api/callbacks/\"><i>Callbacks</i> de Keras</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de la evolución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La evolución del modelo se puede estudiar mediante TensorBoard, o con el historial del entrenamiento.\n",
    "* Para usar TensorBoard hay que almacenar la traza de Keras mediante un *callback* específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "history = model.fit(x_tr, y_tr, epochs=10, validation_split=0.1, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización mediante Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización mediante TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evitar el sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uno de los problemas de las Redes Neuronales Profundas (*Deep Neural Networks*, DNNs) es su tendencia al sobreajuste, debido al alto número de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_tr, y_tr, validation_split=0.9, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Dada la evolución de los errores representada en la siguiente celda, discutir si se distingue sobreajuste del modelo.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dropout*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una aproximación típica para regularizar una DNN es el *dropout*.\n",
    "* Durante el entrenamiento, un cierto porcentaje $r$ de las entradas a una cierta capa se ponen a $0$, mientras las otras se multiplican por $\\frac{1}{1 - r}$ para compensar el cambio de escala.\n",
    "* La red «aprende» a distribuir el procesamiento de la información, en lugar de confiar en unidades sueltas.\n",
    "* Durante la predicción sobre nuevos datos se usan todas las unidades de manera usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código genera un problema sencillo de regresión en una dimensión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "n_pat = 64\n",
    "lim = 3\n",
    "noise = 1e0\n",
    "x_orig = np.linspace(- lim, lim, n_pat)\n",
    "x_long = np.linspace(- lim, lim, 10 * n_pat)\n",
    "y = np.square(x_orig) + noise * np.random.randn(n_pat)\n",
    "x = x_orig.reshape(-1, 1)\n",
    "\n",
    "plt.plot(x_orig, y, \"*\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Analizar la influencia de las capas de Dropout usadas en la siguiente celda.\n",
    "* Discutir si estas capas están evitando o no el sobreajuste.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in (0.0, 0.2, 0.4, 0.6, 0.8):\n",
    "    inputs = keras.Input(shape=(1))\n",
    "    layers = keras.layers.Dense(100, activation=\"relu\")(inputs)\n",
    "    layers = keras.layers.Dropout(d)(layers)\n",
    "    layers = keras.layers.Dense(100, activation=\"relu\")(layers)\n",
    "    layers = keras.layers.Dropout(d)(layers)\n",
    "    layers = keras.layers.Dense(100, activation=\"relu\")(layers)\n",
    "    outputs = keras.layers.Dense(1)(layers)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        metrics=[\"mean_squared_error\"],\n",
    "    )\n",
    "\n",
    "    epochs = 1000\n",
    "    print(\"Fitting NN with dropout at {:2.0f}%...\".format(100 * d))\n",
    "    model.fit(x, y, epochs=epochs, verbose=0)\n",
    "    plt.plot(x_long, model.predict(x_long.reshape(-1, 1), verbose=0), label=\"Dropout {:.0f}%\".format(100 * d))\n",
    "\n",
    "plt.plot(x_orig, y, \"*k\", label=\"Observed\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las técnicas de regularización $L_1$ y $L_2$ son muy utilizadas en el entrenamiento de modelos de ML para evitar el sobreajuste.\n",
    "    * $L_1$ (Lasso Regression): Tiene como objetivo minimizar el valor absoluto de los pesos.\n",
    "    * $L_2$ (Ridge Regression, *weight decay*): Tiene como objetivo minimizar la magnitud al cuadrado de los pesos.\n",
    "* La principal diferencia entre $L_1$ y $L_2$ es que $L_1$ permite discriminar qué características son más importantes en el modelo, llevando a $0$ aquellas características de poco peso. $L_2$ es más eficiente computacionalmente.\n",
    "* En resumen, las técnicas de regularización $L_1$ y $L_2$ consiguen que el modelo tenga menos varianza, lo que ayuda a combatir el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Analizar la influencia de la regularización $L_2$ usada en la siguiente celda.\n",
    "* Discutir si esta regularización están evitando o no el sobreajuste.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in np.logspace(-5, 1, 7):\n",
    "    inputs = keras.Input(shape=(1))\n",
    "    layers = keras.layers.Dense(100, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(l))(inputs)\n",
    "    layers = keras.layers.Dense(100, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(l))(layers)\n",
    "    layers = keras.layers.Dense(100, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(l))(layers)\n",
    "    outputs = keras.layers.Dense(1)(layers)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        metrics=[\"mean_squared_error\"],\n",
    "    )\n",
    "\n",
    "    epochs = 1000\n",
    "    print(\"Fitting NN with regularization at {:.0e}...\".format(l))\n",
    "    model.fit(x, y, epochs=epochs, verbose=0)\n",
    "    plt.plot(x_long, model.predict(x_long.reshape(-1, 1), verbose=0), label=\"Regularization {:.0e}\".format(l))\n",
    "\n",
    "plt.plot(x_orig, y, \"*k\", label=\"Observed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de muestras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Otra manera de evitar el sobreajuste es usar una gran cantidad de datos de entrenamiento.\n",
    "    * No siempre está disponible, dependerá del problema.\n",
    "* Una solución es generar nuevos datos (*data augmentation*).\n",
    "* Este proceso no es trivial, los datos generados tienen que ser relevantes para el problema.\n",
    "* Diferentes aproximaciones:\n",
    "    * Perturbar con ruido.\n",
    "    * Ajustar la distribución de los datos originales.\n",
    "    * Usar conocimiento experto sobre las variaciones en la vida real (algo particularmente útil con imágenes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se carga una imagen de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "\n",
    "china = load_sample_images().images[0]\n",
    "\n",
    "plt.imshow(china)\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda perturba la imagen anterior, generando nuevas muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\")\n",
    "\n",
    "x = keras.preprocessing.image.img_to_array(china)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = 4\n",
    "n_images = n_rows * n_cols\n",
    "\n",
    "i = 0\n",
    "plt.figure(figsize=(15, 15))\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    i += 1\n",
    "    if i > n_images:\n",
    "        break\n",
    "\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    plt.imshow(keras.preprocessing.image.array_to_img(batch[0]))\n",
    "    plt.title(\"Sample {}\".format(i))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Observar los diferentes efectos de distorsión en las imágenes anteriores, identificándolo con las opciones usadas en la generación.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Transfer Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para evitar el sobreajuste, se puede pre-entrenar el modelo en un conjunto de datos grande, y luego adaptarlo al problema en cuestión.\n",
    "* Esta aproximación se conoce como *transfer learning*:\n",
    "    1. Tomar un modelo entrenado con éxito sobre un conjunto de datos grande (el modelo completo, o solo una parte de él).\n",
    "    1. Añadir las capas necesarias para adaptarlo al problema en cuestión.\n",
    "    1. Entrenar las nuevas capas.\n",
    "    1. Entrenar todas las capas con una tasa de aprendizaje pequeña (*fine-tuning*).\n",
    "* Keras facilita esta aproximación.\n",
    "    * Proporciona varias arquitecturas pre-entrenadas.\n",
    "    * Permite seleccionar qué parámetros se entrenarán, dejando los demás congelados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repaso de Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un AutoEncoder (AE) es un tipo de red neuronal que aprende a replicar a la salida exactamente la misma información que recibe en la entrada.\n",
    "* Las capas de entrada y de salida deben tener el mismo número de unidades.\n",
    "* La clave está en la capa oculta, donde la red almacena una representación abstracta de la información.\n",
    "\n",
    "<img src=\"figures/AE.svg\" alt=\"AutoEncoder.\">\n",
    "\n",
    "* El aprendizaje de los AE es no supervisado.\n",
    "* El AE trata de modelar la identidad, $f(\\mathbf{x}_i) = \\mathbf{x}_i$.\n",
    "* El AE puede enfrentarse a este problema de dos formas:\n",
    "    1. Comportamiento deseado: El AE aprende a codificar (y posiblemente comprimir) los datos en la capa oculta.\n",
    "    2. Comportamiento trivial: El AE copia la entrada $\\mathbf{x}_i$ capa a capa, desde la entrada hasta la salida.\n",
    "* Es necesario forzar al AE a comprimir los datos para evitar el comportamiento trivial.\n",
    "* El AE queda divido en dos subredes.\n",
    "    1. El codificador, donde se realiza la extracción de características.\n",
    "    2. El decodificador, donde se invierte la extracción de características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders en Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda se carga el conjunto de dígitos MNIST, que se utilizará para ilustrar varios de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_tr, y_tr), (x_te, y_te) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"Number of axis:   \", x_tr.ndim)\n",
    "print(\"Dimension (train):\", x_tr.shape)\n",
    "print(\"Dimension (test): \", x_te.shape)\n",
    "print(\"Data type:        \", x_tr.dtype)\n",
    "\n",
    "plt.imshow(x_tr[0])\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# The pixels are transformed to the interval [0, 1].\n",
    "x_tr = x_tr.astype(\"float32\") / 255.\n",
    "x_te = x_te.astype(\"float32\") / 255.\n",
    "\n",
    "# Each image is converted into a 1-dimensional vector.\n",
    "x_tr_1D = x_tr.reshape(len(x_tr), -1)\n",
    "x_te_1D = x_te.reshape(len(x_te), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructor del Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para construir un AE en Keras basta con determinar la arquitectura:\n",
    "    * Capa de entrada, correspondiente a los datos que se van a codificar.\n",
    "    * Capas del codificador, encargadas de comprimir la información.\n",
    "    * Capas del decodificador, encargadas de descomprimir la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_builder(inp_lay, enc_lays, dec_lays, optimizer=\"adam\"):\n",
    "    # AE.\n",
    "    autoencoder = keras.Sequential([inp_lay] + enc_lays + dec_lays)\n",
    "    autoencoder.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "    # Encoder.\n",
    "    encoder = keras.Sequential([inp_lay] + enc_lays)\n",
    "\n",
    "    # Decoder.\n",
    "    decoder = keras.Sequential([keras.Input(shape=enc_lays[-1].output_shape[1:])] + dec_lays)\n",
    "\n",
    "    return [autoencoder, encoder, decoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders simples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una primera forma de garantizar que el AE comprime la información es forzando a la capa oculta a tener una dimensión mucho menor que la dimensión de entrada.\n",
    "\n",
    "<img src=\"figures/AESmall.svg\" alt=\"AutoEncoder con dimensión reducida.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Completar la siguiente celda para definir la capa de entrada (`inp_lay`), las capas del codificador (`enc_lays`) y las capas del decodificador (`dec_lays`), de manera que en el AE resultante:\n",
    "    * La capa de entrada tome un vector de dimensión $784$ ($28 \\times 28$).\n",
    "    * El codificador tenga una única capa densa de tamaño `encoding_dim` (correspondiente a la dimensión reducida) con función de activación ReLU.\n",
    "    * El decodificador deshaga la codificación restaurando los datos a la dimensión original ($784$), con función de activación sigmoidal.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 16\n",
    "\n",
    "################################\n",
    "# Insertar código.\n",
    "inp_lay = keras.Input(shape=(x_tr_1D.shape[1],))\n",
    "enc_lays = [keras.layers.Dense(encoding_dim, activation=\"relu\")]\n",
    "dec_lays = [keras.layers.Dense(x_tr_1D.shape[1], activation=\"sigmoid\")]\n",
    "################################\n",
    "\n",
    "[autoencoder, encoder, decoder] = autoencoder_builder(inp_lay,\n",
    "                                                      enc_lays,\n",
    "                                                      dec_lays)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El AE se entrena simplemente minimizando el error cuadrático, como cualquier otra red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(x_tr_1D, x_tr_1D, epochs=10, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las imágenes se pueden codificar y decodificar aplicando a los datos originales las subredes del codificador y del decodificador, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = encoder.predict(x_te_1D, verbose=0)\n",
    "decoded_imgs = decoder.predict(encoded_imgs, verbose=0)\n",
    "print(\"Prediction error: {:.3f}\".format(autoencoder.evaluate(x_te_1D, x_te_1D, verbose=0)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstrucción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos ejemplos de las imágenes originales y reconstruidas se muestran a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_te[i].reshape(28, 28))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Embedding*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se puede aplicar solo el codificador para realizar una reducción de dimensión de los datos iniciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.scatter(encoded_imgs[y_te==i, 0], encoded_imgs[y_te==i, 1], label=\"Digit {}\".format(i))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Replicar el procedimiento anterior fijando la dimensión reducida a $2$.\n",
    "* ¿Tiene suficiente expresividad el AE?\n",
    "* ¿El *embedding* resultante es mejor o peor?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders *sparse*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una forma de garantizar la comprensión de información cuando la capa oculta es más grande que la de entrada es forzar a que los AE produzcan una codificación *sparse* de los datos (Sparse AE).\n",
    "    * Se utiliza una regularización que induzca dispersión en la capa oculta.\n",
    "* Solo se activan un subconjunto de las unidades ocultas a la vez.\n",
    "\n",
    "<img src=\"figures/AESparse.svg\" alt=\"Sparse AutoEncoder.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Completar la siguiente celda para definir la capa de entrada (`inp_lays`), las capas del codificador (`enc_lays`) y las capas del decodificador (`dec_lays`), de manera que en el AE resultante:\n",
    "    * La capa de entrada tome un vector de dimensión $784$ ($28 \\times 28$).\n",
    "    * El codificador tenga una única capa densa de tamaño `encoding_dim` (correspondiente a la dimensión extendida) con función de activación ReLU. Esta capa tendrá además una regularización para que la salida sea dispersa.\n",
    "    * El decodificador deshaga la codificación restaurando los datos a la dimensión original ($784$), con función de activación sigmoidal.\n",
    "\n",
    "<div class=\"notes\">\n",
    "\n",
    "* Para aplicar una regularización a la salida de una capa basta usar la opción `activity_regularizer` de la capa. En concreto, para que se induzca dispersión se puede usar una regularización de tipo $L_1$ (e.g. `activity_regularizer=keras.regularizers.l1(1e-3)`).\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 800\n",
    "\n",
    "################################\n",
    "# Insertar código.\n",
    "inp_lay = keras.Input(shape=(x_tr_1D.shape[1],))\n",
    "enc_lays = [keras.layers.Dense(encoding_dim,\n",
    "                               activation=\"relu\",\n",
    "                               activity_regularizer=keras.regularizers.l1(1e-3))]\n",
    "dec_lays = [keras.layers.Dense(x_tr_1D.shape[1], activation=\"sigmoid\")]\n",
    "################################\n",
    "\n",
    "[autoencoder, encoder, decoder] = autoencoder_builder(inp_lay,\n",
    "                                                      enc_lays,\n",
    "                                                      dec_lays)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El entrenamiento del Sparse AE garantizará que solo un subconjunto de las unidades se activan a la vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(x_tr_1D, x_tr_1D, epochs=10, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = encoder.predict(x_te_1D, verbose=0)\n",
    "decoded_imgs = decoder.predict(encoded_imgs, verbose=0)\n",
    "print(\"Prediction error: {:.3f}\".format(autoencoder.evaluate(x_te_1D, x_te_1D, verbose=0)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstrucción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_te[i].reshape(28, 28))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Embedding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.scatter(encoded_imgs[y_te==i, 0], encoded_imgs[y_te==i, 1], label=\"Digit {}\".format(i))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La codificación de un Sparse AE es dispersa, por tanto muchas de las coordenadas son iguales a $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sparsity: {:.2f}%\".format(100 * (encoded_imgs == 0).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* ¿Qué sucede si se pone a $0$ la regularización?\n",
    "* ¿Se comprime realmente la información?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders profundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una ampliación de los AEs estándar es usar AEs profundos.\n",
    "    * Una DNN realiza la codificación.\n",
    "    * Una DNN (normalmente simétrica a la anterior) realiza la decodificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Completar la siguiente celda para definir la capa de entrada (`inp_lay`), las capas del codificador (`enc_lays`) y las capas del decodificador (`dec_lays`), de manera que en el AE resultante:\n",
    "    * La capa de entrada tome un vector de dimensión $784$ ($28 \\times 28$).\n",
    "    * El codificador tenga tres capas completamente conectadas con $128$, $64$ y $2$ unidades y activación ReLU.\n",
    "    * El decodificador deshaga la codificación restaurando los datos a la dimensión original ($784$), usando una arquitectura simétrica a la anterior, también de tres capas (las dos primeras con activación ReLU, y la última con activación sigmoidal).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Insertar código.\n",
    "inp_lay = keras.Input(shape=(784,))\n",
    "enc_lays = [\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(2, activation=\"relu\"),\n",
    "]\n",
    "dec_lays = [\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(784, activation=\"sigmoid\"),\n",
    "]\n",
    "################################\n",
    "\n",
    "[autoencoder, encoder, decoder] = autoencoder_builder(inp_lay,\n",
    "                                                      enc_lays,\n",
    "                                                      dec_lays)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para los Deep AEs se usa un entrenamiento similar al estándar en DNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(x_tr_1D, x_tr_1D, epochs=10, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = encoder.predict(x_te_1D, verbose=0)\n",
    "decoded_imgs = decoder.predict(encoded_imgs, verbose=0)\n",
    "print(\"Prediction error: {:.3f}\".format(autoencoder.evaluate(x_te_1D, x_te_1D, verbose=0)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstrucción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_te[i].reshape(28, 28))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Embedding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.scatter(encoded_imgs[y_te==i, 0], encoded_imgs[y_te==i, 1],label=\"Digit {}\".format(i))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Considerando que la dimensión de codificación es $2$, ¿el *embedding* resultante es mejor o peor que los anteriores? ¿Por qué?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repaso de Redes Neuronales Convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los modelos de ML suelen ser sensibles ante traslaciones de las entradas, de manera que un cambio como\n",
    "$$ \\begin{pmatrix} 0 & 1 & 3 & 5 & 0 & 0 \\end{pmatrix} \\to \\begin{pmatrix} 0 & 0 & 1 & 3 & 5 & 0 \\end{pmatrix} $$\n",
    "altera completamente las predicciones.\n",
    "* Este tipo de perturbaciones son muy comunes en algunos problemas reales, como los relacionados con imágenes.\n",
    "    * Dos imágenes con una ligera traslación deberían dar una salida similar.\n",
    "    * Sin embargo, en DNN estándar este no es el caso.\n",
    "* Las Redes Neuronales Convolucionales (*Convolutional Neural Networks*, CNNs) afrontan este problema mediante:\n",
    "    * Capas de convolución, que aplican filtros sobre las imágenes, detectando diferentes estructuras.\n",
    "    * Capas de *pooling*, que reducen la dimensión y proporcionan invarianza ante traslaciones.\n",
    "* La diferencia fundamental con respecto a los métodos de filtrado de imágenes tradicionales es que en las CNN los filtros no se fijan de antemano, sino que se ajustan durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolución de imágenes en Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagen original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda carga una imagen de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "china = datasets.load_sample_images().images[0]\n",
    "china = china[:china.shape[0], :china.shape[0], :] / 255.0\n",
    "plt.imshow(china)\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolución con diferentes filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aunque la potencia de las CNN es que los filtros se aprenden de los datos, en Keras se puede inicializar el valor a mano de las capas convolucionales para ver el efecto de distintos filtros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = []\n",
    "kv.append(np.array([[0, 0, 0],\n",
    "                    [0, 1, 0],\n",
    "                    [0, 0, 0]]))\n",
    "kv.append(np.array([[1, 1, 1],\n",
    "                    [0, 0, 0],\n",
    "                    [-1, -1, -1]]))\n",
    "kv.append(kv[-1].T)\n",
    "kv.append(np.array([[-1, -1, -1],\n",
    "                    [-1, 8, -1],\n",
    "                    [-1, -1, -1]]))\n",
    "kv.append(np.array([[0, -1, 0],\n",
    "                    [-1, 5, -1],\n",
    "                    [0, -1, 0]]))\n",
    "kv.append(1 / 256 * np.array([[1, 4, 6, 4, 1],\n",
    "                              [4, 16, 24, 16, 4],\n",
    "                              [6, 24, 36, 24, 6],\n",
    "                              [4, 16, 24, 16, 4],\n",
    "                              [1, 4, 6, 4, 1]]))\n",
    "lv = (\"Identity\", \"Edge H\", \"Edge V\", \"Edges\", \"Sharpen\", \"Gaussian\")\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "inp = tf.constant([china])\n",
    "for k, l in zip(kv, lv):\n",
    "    def kernel_init(shape, dtype=None):\n",
    "        kernel = np.zeros(shape)\n",
    "        kernel[:, :, 0, 0] = k\n",
    "        kernel[:, :, 1, 1] = k\n",
    "        kernel[:, :, 2, 2] = k\n",
    "        return kernel\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Conv2D(3,\n",
    "                                                  k.shape,\n",
    "                                                  kernel_initializer=kernel_init,\n",
    "                                                  input_shape=china.shape)])\n",
    "    model.build()\n",
    "    out = model.predict(inp, verbose=0)[0]\n",
    "    out = np.clip(out, 0, 1)\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(china)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(k)\n",
    "    plt.title(\"Kernel ({})\".format(l))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(out)\n",
    "    plt.title(\"Convoluted Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* ¿Qué efecto producirá un filtro formado por una matriz de tamaño $20 \\times 20$ con un valor contante de $\\frac{1}{400}$?\n",
    "* Comprobar el efecto producido modificando el código anterior.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Insertar código.\n",
    "k = 1. / 400 * np.ones((20, 20))\n",
    "def kernel_init(shape, dtype=None):\n",
    "        kernel = np.zeros(shape)\n",
    "        kernel[:, :, 0, 0] = k\n",
    "        kernel[:, :, 1, 1] = k\n",
    "        kernel[:, :, 2, 2] = k\n",
    "        return kernel\n",
    "\n",
    "model = keras.Sequential([keras.layers.Conv2D(3,\n",
    "                                              k.shape,\n",
    "                                              kernel_initializer=kernel_init,\n",
    "                                              input_shape=china.shape)])\n",
    "model.build()\n",
    "out = model.predict(inp, verbose=0)[0]\n",
    "out = np.clip(out, 0, 1)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(china)\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(k)\n",
    "plt.title(\"Kernel (Constant)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(out)\n",
    "plt.title(\"Convoluted Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neuronales Convolucionales en Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se modifica el conjunto de datos de MNIST para que cada ejemplo tenga dimensión $28 \\times 28 \\times 1$, ya que las capas convolucionales de Keras esperan que la última dimensión sea el canal (en este caso solo hay 1, ya que son imágnes en escala de grises)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = x_tr.reshape(-1, 28, 28, 1)\n",
    "x_te = x_te.reshape(-1, 28, 28, 1)\n",
    "\n",
    "y_tr = keras.utils.to_categorical(y_tr, num_classes=10)\n",
    "y_te = keras.utils.to_categorical(y_te, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neuronales Convolucionales profundas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En Keras se pueden definir fácilmente las CNN usando las capas convolucionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.Sequential()\n",
    "\n",
    "cnn.add(keras.layers.Conv2D(32, kernel_size=(3,3),  activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "cnn.add(keras.layers.Conv2D(64, kernel_size=(3,3), activation=\"relu\"))\n",
    "cnn.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(keras.layers.Dropout(0.2))\n",
    "cnn.add(keras.layers.Flatten())\n",
    "cnn.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "cnn.add(keras.layers.Dropout(0.4))\n",
    "cnn.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Comprobar el número de parámetros libres de la segunda capa.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Insertar código.\n",
    "print(\"No. parameters (layer 1):\", 32 * (1 * 3 * 3 + 1))\n",
    "print(\"No. parameters (layer 2):\", 64 * (32 * 3 * 3 + 1))\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El entrenamiento de la CNN es similar al de cualquier otra DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.fit(x_tr, y_tr, validation_split=0.75, batch_size=256, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test accuracy: {:.3f}%\".format(100 * cnn.evaluate(x_te, y_te, verbose=0)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se puede pintar la evolución de los errores para detectar posibles problemas de sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "preds = cnn.predict(x_te, verbose=0)\n",
    "y_te_t = np.argmax(y_te, axis=1)\n",
    "y_te_p = np.argmax(preds, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_te_t, y_te_p)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sn.heatmap(cm, annot=True)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.axis(\"equal\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repaso de Redes Neuronales Recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La mayoría de modelos de ML asumen independencia entre los ejemplos.\n",
    "* Hay muchas situaciones en los que esta hipótesis no es cierta, y los patrones son muy dependientes del contexto:\n",
    "    * Predicción de series temporales.\n",
    "    * Procesamiento del lenguaje natural.\n",
    "* Las Redes Neuronales Recurrentes (*Recurrent Neural Networks*, RNNs) tratan de paliar este problema a través de conexiones *backward*.\n",
    "* En concreto, las unidades *Long Short-Term Memory* (LSTM) son componentes diseñadas para retener información de patrones anteriores, a más largo plazo que otras RNNs estándar.\n",
    "* Estas unidades suelen tener los siguientes componentes:\n",
    "    * Celda, que constituye la memoria de la unidad LSTM.\n",
    "    * Puerta de entrada, que controla la influencia de la nueva entrada en la celda.\n",
    "    * Puerta de olvido, que controla cuánto se mantiene el valor en la celda.\n",
    "    * Puerta de salida, que controla cuánto se usa el valor de la celda para calcular la salida de la unidad.\n",
    "* Hay conexiones de entrada y salida a la LSTM (algunas recurrentes).\n",
    "* Los pesos de todas estas conexiones (incluidas las puertas) se aprenden durante el entrenamiento.\n",
    "* La red aprende qué patrones debe retener.\n",
    "\n",
    "<img src=\"figures/LSTM.svg\" alt=\"Unidad LSTM.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neuronales Recurrentes en Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se genera una serie temporal sencilla como ejemplo para ilustrar las RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(- 8 * np.pi, 8 * np.pi, 513)\n",
    "x = np.sin(x)\n",
    "\n",
    "y = x[1:].reshape(- 1, 1)\n",
    "x = x[:-1].reshape(- 1, 1, 1)\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(x, y, test_size=0.3, shuffle=False)\n",
    "plt.plot(range(len(y_tr.ravel())), y_tr.ravel())\n",
    "plt.plot(range(len(y_tr.ravel()), len(y_tr.ravel()) + len(y_te.ravel())), y_te.ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neuronales Recurrentes LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En Keras existe un tipo de capa LSTM, que incluye tantas unidades LSTM como se especifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = keras.Sequential()\n",
    "rnn.add(keras.layers.LSTM(50, batch_input_shape=(1, 1, 1), stateful=True))\n",
    "rnn.add(keras.layers.Dense(1))\n",
    "\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(loss=\"mean_squared_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_rnn = rnn.fit(x_tr, y_tr, epochs=10, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.reset_states()\n",
    "rnn.predict(x_tr, batch_size=1, verbose=0)\n",
    "preds_rnn = rnn.predict(x_te, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_te.ravel(), label=\"Real\")\n",
    "plt.plot(preds_rnn.ravel(), label=\"Pred\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_te, y_te)\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_te, preds_rnn)\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Pred\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Observando la predicción anterior, ¿la salida de la RNN depende solo de su entrada (es decir, el valor en el instante anterior) o depende también del contexto? ¿Por qué?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Generativas Adversarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a las Redes Generativas Adversarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una aplicación no supervisada de las DNNs consiste en generar nuevas muestras, siguiendo una distribución previamente aprendida.\n",
    "    * Por ejemplo, generar caras realistas a partir de una base de datos de caras.\n",
    "* Surgen dos preguntas fundamentales:\n",
    "    1. ¿Cómo entrenar este tipo de modelos?\n",
    "    1. ¿Qué entrada usar para generar los nuevos ejemplos?\n",
    "* Las Redes Generativas Adversarias (*Generative Adversarial Networks*, GANs) afrontan este problema del siguiente modo:\n",
    "    1. Se sigue una aproximación adversaria para entrenar dos redes en un juego competitivo.\n",
    "    1. Se usa ruido aleatorio para generar las muestras.\n",
    "* Las GANs están compuestas por dos redes: una generadora y otra discriminatoria.\n",
    "* La red generadora recibe como entrada ruido, y produce como salida muestras de la distribución modelada.\n",
    "* La red discriminatoria recibe como entrada imágenes, y como salida trata de distinguir si son imágenes reales o generadas por la red generadora.\n",
    "* La magia de las GANs se produce en el entrenamiento adversario, donde se repite el siguiente proceso:\n",
    "    1. Se usa la generadora para producir imágenes falsas.\n",
    "    1. Se entrena la discriminadora para distinguir entre las imágenes falsas y las reales.\n",
    "    1. Se entrena la generadora para maximizar el error de la discriminadora.\n",
    "\n",
    "<img src=\"figures/GAN.svg\" alt=\"GAN.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Generativas Adversarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El discriminador es simplemente una red profunda que toma como entrada una imagen, y realiza una clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    discriminator = keras.Sequential()\n",
    "\n",
    "    discriminator.add(keras.Input(shape=(28, 28, 1)))\n",
    "    discriminator.add(keras.layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    discriminator.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    discriminator.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(keras.layers.Conv2D(128, kernel_size=4, strides=1, padding=\"same\"))\n",
    "    discriminator.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(keras.layers.Flatten())\n",
    "    discriminator.add(keras.layers.Dropout(0.2))\n",
    "    discriminator.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "discriminator = create_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El generador es una red profunda que toma un vector (aleatorio) de una cierta dimensión, y produce como salida una imagen con el tamaño especificado.\n",
    "* Es común que su arquitectura sea simétrica a la del disciminador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    generator = keras.Sequential()\n",
    "    \n",
    "    generator.add(keras.Input(shape=(100, )))\n",
    "    generator.add(keras.layers.Dense(7 * 7 * 128))\n",
    "    generator.add(keras.layers.Reshape((7, 7, 128)))\n",
    "    generator.add(keras.layers.Conv2DTranspose(128, kernel_size=4, strides=1, padding=\"same\"))\n",
    "    generator.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    generator.add(keras.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    generator.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    generator.add(keras.layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    generator.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    generator.add(keras.layers.Conv2D(1, kernel_size=5, padding=\"same\", activation=\"sigmoid\"))\n",
    "    \n",
    "    return generator\n",
    "\n",
    "generator = create_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La GAN es la concatenación del generador y el discriminador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gan(discriminator, generator):\n",
    "    gan_input = keras.Input(shape=(100,))\n",
    "    gan = keras.Model(inputs=gan_input, outputs=discriminator(generator(gan_input)))\n",
    "    gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "    return gan\n",
    "\n",
    "gan = create_gan(discriminator, generator)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función permite pintar muestras de las imágenes generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(generator, dim=(5, 5), figsize=(5, 5)):\n",
    "    examples = np.prod(dim)\n",
    "    noise = np.random.normal(loc=0, scale=1, size=[examples, 100])\n",
    "    generated_images = generator.predict(noise, verbose=0)\n",
    "    generated_images = generated_images.reshape(examples, 28, 28)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        image = generated_images[i]\n",
    "\n",
    "        plt.subplot(dim[0], dim[1], i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 101\n",
    "batch_size = 128\n",
    "\n",
    "(x_tr, y_tr), (x_te, y_te) = keras.datasets.mnist.load_data()\n",
    "x = x_tr[y_tr == 4].astype(\"float32\") / 255.\n",
    "\n",
    "for i in range(max_iter):\n",
    "\n",
    "    print(\"Iteration: {}\".format(i), end=\"\\r\")\n",
    "    if (i % 10) == 0:\n",
    "        plot_generated_images(generator)\n",
    "\n",
    "    noise = np.random.normal(0, 1, [batch_size, 100])\n",
    "\n",
    "    generated_images = generator.predict(noise, verbose=0)\n",
    "    real_images = x[np.random.randint(low=0, high=x.shape[0], size=batch_size)]\n",
    "\n",
    "    X = np.concatenate([real_images, generated_images[:, :, :, 0]])\n",
    "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "\n",
    "    y_dis = np.zeros(2 * batch_size)\n",
    "    y_dis[:batch_size] = 1.0\n",
    "\n",
    "    discriminator.trainable=True\n",
    "    discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "    noise = np.random.normal(0, 1, [batch_size, 100])\n",
    "    y_gen = np.ones(batch_size)\n",
    "\n",
    "    discriminator.trainable=False\n",
    "    gan.train_on_batch(noise, y_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Analizar el entrenamiento de la GAN.\n",
    "* ¿Por qué se modifican las etiquetas de los patrones?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción al Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Además del Aprendizaje Supervisado y el Aprendizaje No Supervisado, existe otro paradigma denominado Aprendizaje por Refuerzo (*Reinforcement Learning*, RL), que trata de determinar qué acciones debe tomar un agente en un cierto entorno para maximizar una recompensa.\n",
    "* Este tipo de problemas suele formalizarse como un Proceso de Markov de Decisión:\n",
    "    * $s_t$ es el estado a tiempo $t$.\n",
    "    Algunos estados son terminales, en el sentido de que terminan el episodio.\n",
    "    * $r_t$ es la recompensa obtenida a tiempo $t$.\n",
    "    * $a_t$ es la acción que toma el agente a tiempo $t$.\n",
    "    * $p_s(s_2 | s_1, a)$ es el modelo de transición, que modela la probabilidad de ir del estado $s_1$ al estado $s_2$ cuando se toma la acción $a$.\n",
    "    * $p_r(r | s, a)$ es el modelo de recompensa, que modela la probabilidad de obtener una recompensa $r$ desde el estado $s$ cuando se toma la acción $a$.\n",
    "* El objetivo del RL es encontrar una política, una probabilidad $\\pi(a | s)$ que maximice la recompensa esperada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolución de problemas de RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Si las probabilidades reales $p_s$ y $p_r$ se conocen, entonces la política óptima $\\pi(a | s)$ puede calcularse mediante programación dinámica.\n",
    "* En general estas distribuciones son desconocidas, así que se pueden seguir dos aproximaciones distintas:\n",
    "    1. Métodos basados en modelo, que tratan de modelar $p_s$ y $p_r$, y luego estimar $\\pi(a | s)$ a partir de estos modelos.\n",
    "    1. Métodos *model-free*, que tratan directamente de optimizar la política $\\pi(a | s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este método el agente aprende a predecir, a partir del estado actual, dos salidas distintas:\n",
    "    1. La política, $\\pi(a | s)$. La parte del agente encargada de esta salida es el actor.\n",
    "    1. La recompensa estimada en el futuro. La parte del agente encargada de esta salida es el crítico.\n",
    "* El crítico aprende comparando la recompensa obtenida en episodios reales con su predicción, y haciendo descenso por gradiente.\n",
    "* Por otro lado, el actor utiliza *policy gradient* (estima el gradiente mediante el método de Montecarlo), modificado para usar la información producida por el crítico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje por Refuerzo en Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de datos: CartPole v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Este problema consiste en un poste articulado sobre un carro, que se mueve por una vía.\n",
    "* El poste comienza vertical, y el objetivo es impedir que caiga cambiando la velocidad del carro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Índice| Observación| Mín | Máx|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| 0 | Posición del Carro | $$-2.4$$ | $$2.4$$ |\n",
    "| 1 | Velocidad del Carro  | $$-\\infty$$ | $$\\infty$$ |\n",
    "| 2 | Ángulo del Poste | $$\\sim -41.8^\\circ$$ | $$\\sim 41.8^\\circ$$ |\n",
    "| 3 | Velocidad del Poste (en el Extremo) | $$-\\infty$$ | $$\\infty$$ |\n",
    "\n",
    "* El estado inicial es aleatorio, según una uniforme $\\pm 0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acciones y recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Índice | Acción |\n",
    "|:-:|:-:|\n",
    "| 0 | Empujar el carro hacia la izquierda |\n",
    "| 1 | Empujar el carro hacia la derecha|\n",
    "\n",
    "* La recompensa es $1$ para cada instante de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fin del episodio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El episodio termina si se cumple cualquiera de las siguientes condiciones:\n",
    "    1. El ángulo del poste es mayor que $\\pm 12^\\circ$.\n",
    "    2. La posición del carro es mayor que $\\pm 2.4$ (el centro del carro abandona la imagen).\n",
    "    3. El episodio dura más de $500$ pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Política aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda crea el entorno del problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código genera un episodio siguiendo la política especificada (si no hay modelo, se selecciona la siguiente acción aleatoriamente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(env, early_stop=True, model=None, n_steps=500):\n",
    "    state = env.reset()\n",
    "    for i in range(n_steps):\n",
    "        env.render()\n",
    "        if model is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action_probs, _ = model.predict(tf.expand_dims(tf.convert_to_tensor(state), 0), verbose=0)\n",
    "            p = np.squeeze(action_probs)\n",
    "            action = np.random.choice(len(p), p=p)\n",
    "        state, _, done, _ = env.step(action)\n",
    "        if early_stop and done:\n",
    "            print(\"Finished after {} steps\".format(i))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence(env, model=None, early_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método *Actor-Critic* con Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta celda configura el modelo.\n",
    "El parámetro $\\gamma$ (variable `gamma`) determina el olvido o factor de descuento de las recompensas pasadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "max_steps_per_episode = 10000\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda construye la red neuronal que implementará tanto el actor como el crítico, compartiendo la entrad y la capa oculta, y usando una capa de salida diferente para la política (actor) y la estimación de recompensa (crítico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "num_hidden = 128\n",
    "\n",
    "inputs = keras.layers.Input(shape=(num_inputs,))\n",
    "common = keras.layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = keras.layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = keras.layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* ¿Cuál es la función de activación de cada capa de salida?\n",
    "¿Por qué?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El entrenamiento consiste básicamente en completar episodios, acumulando la pérdida tanto del actor como del crítico a lo largo de todos los pasos de un episodio.\n",
    "* Durante cada episodio, se sigue la política aprendida hasta el momento.\n",
    "* Cuando finaliza el episodio, se actualizan los pesos del agente a partir del gradiente de ambas pérdidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            # Show the attemps.\n",
    "            env.render()\n",
    "\n",
    "            # Estimate the policy (prediction of the next actions) and the future rewards using the model.\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Choose random action using the policy.\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # Apply the sampled action.\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Once the episode is finished, update running reward to check condition for solving.\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate the real expected value from rewards.\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[:-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Compute the loss values (both for Actor and Critic) to update the network.\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)\n",
    "            critic_losses.append(huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)))\n",
    "\n",
    "        # Update the weights through backpropagation.\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear variables.\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        print(\"Running reward: {:6.2f} at episode {:3d}\".format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 80:\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado, el modelo es capaz de generar secuencias mucho más largas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence(env, model=model)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "291.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
